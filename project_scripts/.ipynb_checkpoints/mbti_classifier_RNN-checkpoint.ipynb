{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "mJYuGsKFeGUz",
    "outputId": "16727bbe-5a32-46fc-89e2-24cd7570a3ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'18/37 @.@|||Science  is not perfect. No scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'No, I can't draw on my own nails (haha). Thos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'I tend to build up a collection of things on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>I'm not sure, that's a good question. The dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=w8-egj0y8Qs||...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...\n",
       "5  INTJ  '18/37 @.@|||Science  is not perfect. No scien...\n",
       "6  INFJ  'No, I can't draw on my own nails (haha). Thos...\n",
       "7  INTJ  'I tend to build up a collection of things on ...\n",
       "8  INFJ  I'm not sure, that's a good question. The dist...\n",
       "9  INTP  'https://www.youtube.com/watch?v=w8-egj0y8Qs||..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3ksc7-r5nDNh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This will load the csv\n",
    "'''\n",
    "class CsvToDf:\n",
    "    '''\n",
    "    This class will simply turn the given data to a dataframe\n",
    "    '''\n",
    "    def __init__(self,filename,batchSize=None,cols=None):\n",
    "        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n",
    "        #into memory\n",
    "        self._cols = cols\n",
    "        self._header = None\n",
    "        self._filename = filename\n",
    "        self._curIndex = 0     #this will be the current index that we are in the csv\n",
    "        self._isRead = False\n",
    "        self._df = None\n",
    "        self._storeHeader()\n",
    "        self._batchSize = batchSize\n",
    "    def _storeHeader(self):\n",
    "        with open(self._filename) as csvFile:\n",
    "            f = csv.reader(csvFile)\n",
    "            self._header = next(f)\n",
    "    def getWholeCsv(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename)\n",
    "            self._isRead = True\n",
    "        return self._df\n",
    "    def getHeader(self):\n",
    "        return self._header\n",
    "    def _checkIfRead(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n",
    "            self._isRead = True\n",
    "            return False\n",
    "        return True\n",
    "    def getNextBatchCsv(self):\n",
    "        self._checkIfRead()\n",
    "        return next(self._df,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ counting the smallest number of data\n",
    "TEST = \"test\"\n",
    "TRAIN = \"train\"\n",
    "class Combiner:\n",
    "    '''\n",
    "    - Given multiple CsvToDf that correspond to a dataset combine them to a single dataframe\n",
    "    - return this dataframe\n",
    "    - need to return a dataframe that only has type and post as its columns\n",
    "    '''\n",
    "    def __init__(self,dataList,columnList):\n",
    "        '''\n",
    "        dataList is the CsvToDf that contains all the data and columnList is a list that contains the necessary\n",
    "        column names for a corresponding entry in dataList.\n",
    "        '''\n",
    "        assert len(dataList) == len(columnList),\"incorrect sizes for data\"\n",
    "        self._dataList = dataList\n",
    "        self._data = [None for i in range(len(dataList))]\n",
    "        self._necessaryCol = columnList\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._incrementData()\n",
    "    def getNextBatch(self):\n",
    "        '''\n",
    "        return a dataframe that contains all the aggregated data\n",
    "        '''\n",
    "        outData = pd.DataFrame(columns=[self._typeCol,self._postCol])\n",
    "        for data,colList in zip(self._data,self._necessaryCol):\n",
    "            if isinstance(data,pd.DataFrame):\n",
    "                renamedData = data[[colList[0],colList[1]]]\n",
    "                renamedData.columns = [self._typeCol,self._postCol]\n",
    "                \n",
    "                outData = outData.append(renamedData,ignore_index=True)\n",
    "        self._incrementData()\n",
    "        if (len(outData.index)) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return outData\n",
    "    def _incrementData(self):\n",
    "        for idx,i in enumerate(self._dataList):\n",
    "            self._data[idx] = i.getNextBatchCsv()\n",
    "class Balancer:\n",
    "    '''\n",
    "    - Balance the count\n",
    "    - Decide what the training and test dat will be\n",
    "    - Needs to output three data frames the train the test and the remainder\n",
    "    - make the remainder the training set\n",
    "    '''\n",
    "    def __init__(self,combiner,trainFreq,testFreq):\n",
    "        #personSize is minimum size of the number of people in a single personality group\n",
    "        self._combiner = combiner\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "        self._trainFreq = trainFreq\n",
    "        self._testFreq = testFreq\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "    def createDataSets(self):\n",
    "        self.reset()\n",
    "        while not(self._trainIsUniform()) or not(self._testIsUniform()):\n",
    "            #the three conditionals above will check if test and train dataset have uniform data \n",
    "            batch = self._combiner.getNextBatch()\n",
    "            if not(isinstance(batch,pd.DataFrame)):\n",
    "                break\n",
    "            for idx,row in batch.iterrows():\n",
    "                if isinstance(row[self._typeCol],str):\n",
    "                    personality = row[self._typeCol].upper()\n",
    "                    if personality in self._personality_count:\n",
    "                        if self._personality_count[personality][TRAIN] < self._trainFreq:\n",
    "                            self._training.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TRAIN] += 1\n",
    "                        elif self._personality_count[personality][TEST] < self._testFreq:\n",
    "                            self._testing.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TEST] += 1\n",
    "        return True\n",
    "    def reset(self):\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "    def getTrainSet(self):\n",
    "        return pd.DataFrame(self._training)\n",
    "    def getTestSet(self):\n",
    "        return pd.DataFrame(self._testing)\n",
    "    def _trainIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TRAIN] < self._trainFreq:\n",
    "                return False\n",
    "        return True\n",
    "    def _testIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TEST] < self._testFreq:\n",
    "                return False\n",
    "        return True\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTJ': 231, 'INTJ': 1091, 'ENTP': 685, 'INTP': 1304, 'INFJ': 1470, 'INFP': 1832, 'ENFJ': 190, 'ENFP': 675, 'ESTP': 89, 'ESTJ': 39, 'ISTP': 337, 'ISTJ': 205, 'ISFJ': 166, 'ISFP': 271, 'ESFJ': 42, 'ESFP': 48}\n",
      "{'ENTJ': 358, 'INTJ': 1837, 'ENTP': 624, 'INTP': 2313, 'INFJ': 1023, 'INFP': 1070, 'ENFJ': 206, 'ENFP': 605, 'ESTP': 88, 'ESTJ': 53, 'ISTP': 445, 'ISTJ': 236, 'ISFJ': 134, 'ISFP': 161, 'ESFJ': 34, 'ESFP': 65}\n",
      "{'ENTJ': 13886, 'INTJ': 79680, 'ENTP': 29330, 'INTP': 102763, 'INFJ': 31243, 'INFP': 43876, 'ENFJ': 4460, 'ENFP': 22744, 'ESTP': 3808, 'ESTJ': 1354, 'ISTP': 11679, 'ISTJ': 11470, 'ISFJ': 4036, 'ISFP': 3773, 'ESFJ': 400, 'ESFP': 1583}\n"
     ]
    }
   ],
   "source": [
    "def counter(ctd):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    curCtd = ctd.getNextBatchCsv()\n",
    "    while isinstance(curCtd,pd.DataFrame):\n",
    "        for idx,row in curCtd.iterrows():\n",
    "            if isinstance(row[\"type\"],str):\n",
    "                personality = row[\"type\"].upper()\n",
    "                if personality in personality_dict:\n",
    "                    personality_dict[personality] += 1\n",
    "        curCtd = ctd.getNextBatchCsv()\n",
    "    return personality_dict\n",
    "print(counter(file1))\n",
    "print(counter(file2))\n",
    "print(counter(file3))\n",
    "print(counter(file4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_1.csv\",batchSize=400)\n",
    "file2 = CsvToDf(\"../data/mbti9k_comments.csv\",batchSize=100) \n",
    "file3 = CsvToDf(\"../data/typed_posts.csv\",batchSize=100)\n",
    "file4 = CsvToDf(\"../data/typed_comments.csv\",batchSize=100)\n",
    "\n",
    "combine = Combiner([file1,file2,file3,file4],[[\"type\",\"posts\"],[\"type\",\"comment\"],[\"type\",\"title\"],[\"type\",\"comment\"]])\n",
    "balancer = Balancer(combine,100,10)\n",
    "balancer.createDataSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTJ': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INTJ': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ENTP': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INTP': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INFJ': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INFP': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ENFJ': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ENFP': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ESTP': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), 'ESTJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), 'ISTP': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'ISTJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), 'ISFJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), 'ISFP': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), 'ESFJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'ESFP': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "def getPersonalityDict():\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    for idx,keys in enumerate(personality_dict):\n",
    "        oneVec = np.zeros((16,))\n",
    "        oneVec[idx] = 1\n",
    "        personality_dict[keys] = oneVec\n",
    "    return personality_dict\n",
    "print(getPersonalityDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterDf(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "\n",
    "    for idx,row in df.iterrows():\n",
    "        if isinstance(row[\"type\"],str):\n",
    "            personality = row[\"type\"].upper()\n",
    "            if personality in personality_dict:\n",
    "                personality_dict[personality] += 1\n",
    "    return personality_dict\n",
    "counterDf(balancer.getTrainSet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmeDcYQsH8Jd",
    "outputId": "4b3a8878-f7b5-4b10-c2bc-b3070a11dc3d"
   },
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def convertLabels(labelDf):\n",
    "    '''\n",
    "    this will turn the string labels to floats\n",
    "    '''\n",
    "    personality_dict = getPersonalityDict()\n",
    "    type_labels = []\n",
    "    # Go through the array and turn the personality type into its corresponding number\n",
    "    for idx,personality in enumerate(labelDf):\n",
    "        if isinstance(personality,str):\n",
    "            type_labels.append(personality_dict[personality.upper()])\n",
    "    return np.array(type_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "training_posts = balancer.getTrainSet()[\"posts\"]\n",
    "training_labels = convertLabels(balancer.getTrainSet()[\"type\"])\n",
    "testing_posts = balancer.getTestSet()[\"posts\"]\n",
    "testing_labels = convertLabels(balancer.getTestSet()[\"type\"])\n",
    "print(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50LEptZFKXl6",
    "outputId": "566e4359-729c-4d9b-9f93-8d0e8e1f7858"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Only considering the top 10000 most common words\n",
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "# We only want to fit the tokenizer on the training, not the testing\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\n",
    "training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# training_sequences = np.array(training_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# testing_sequences = np.array(testing_sequences)\n",
    "testing_padded = np.array(testing_padded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "AfPpLGS7MunO"
   },
   "outputs": [],
   "source": [
    "#Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# ^^ actually i dont know if that is true\n",
    "embedding_dim = 256\n",
    "'''\n",
    "Embedding layer will always have vocab_size*embedding_dim parameters. Since vocab_size is 10,000 the number of parameters on this layer will always be large\n",
    "'''\n",
    "model = tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(256, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(128),\n",
    "                            tf.keras.layers.Dense(16, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = 'sgd', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6wi-9cvRtM6",
    "outputId": "30be599c-d625-4336-af86-7462bd9b982f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 2016, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 2016, 256)         393984    \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 3,005,328\n",
      "Trainable params: 3,005,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ReGWYoyXKTQ",
    "outputId": "bbb1564b-c082-49a2-a534-9fc48f10cc91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 182s 114ms/sample - loss: 2.7797 - acc: 0.0669\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 179s 112ms/sample - loss: 2.7635 - acc: 0.0800\n",
      "Epoch 3/10\n",
      " 448/1600 [=======>......................] - ETA: 2:12 - loss: 2.7634 - acc: 0.0871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-77ee977bc6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs = num_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 192s 120ms/sample - loss: 2.8906 - acc: 0.0731\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 199s 124ms/sample - loss: 2.8038 - acc: 0.0613\n",
      "Epoch 3/10\n",
      " 192/1600 [==>...........................] - ETA: 3:03 - loss: 2.7852 - acc: 0.0521"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpMLBWYmzYM5"
   },
   "outputs": [],
   "source": [
    "res = np.argmax(model.predict(testing_padded),axis=1)\n",
    "label = np.argmax(testing_labels,axis=1)\n",
    "print(f\"accuracy = {np.mean((label-res) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mbti_classifier_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
