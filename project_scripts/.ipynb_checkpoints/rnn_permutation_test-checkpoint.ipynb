{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "mJYuGsKFeGUz",
    "outputId": "16727bbe-5a32-46fc-89e2-24cd7570a3ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from copy import copy, deepcopy\n",
    "import unidecode \n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "POSTS = \"posts\"\n",
    "TYPE = \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "3ksc7-r5nDNh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This will load the csv\n",
    "'''\n",
    "\n",
    "class CsvToDf:\n",
    "    '''\n",
    "    This class will simply turn the given data to a dataframe\n",
    "    '''\n",
    "    def __init__(self,filename,batchSize=None,cols=None,preProc=False,postCol=False,toReplace=None):\n",
    "        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n",
    "        #into memory\n",
    "        self._toReplace = toReplace\n",
    "        self._preProc = preProc\n",
    "        self._postCol = postCol\n",
    "        self._cols = cols\n",
    "        self._header = None\n",
    "        self._filename = filename\n",
    "        self._curIndex = 0     #this will be the current index that we are in the csv\n",
    "        self._isRead = False\n",
    "        self._df = None\n",
    "        self._storeHeader()\n",
    "        self._batchSize = batchSize\n",
    "    def preprocessing_v1(self,text):\n",
    "        #remove html information\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        processed = soup.get_text(separator=\" \")\n",
    "\n",
    "        #remove http// \n",
    "        processed = re.sub(r\"http\\S+\", \"\", processed)\n",
    "\n",
    "        #remove ||| seperate\n",
    "        processed = re.sub(r'\\|\\|\\|', r' ', processed)\n",
    "\n",
    "        #lower case\n",
    "        processed = processed.lower()\n",
    "\n",
    "        #expand shortened words, e.g. don't to do not\n",
    "        processed = contractions.fix(processed)\n",
    "\n",
    "        #remove accented char\n",
    "        processed = unidecode.unidecode(processed)\n",
    "\n",
    "        #remove white space\n",
    "        #processed = processed.strip()\n",
    "        #processed = \" \".join(processed.split())\n",
    "\n",
    "        # Lemmatizing \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        processed=lemmatizer.lemmatize(processed)\n",
    "        return processed\n",
    "    def _storeHeader(self):\n",
    "        with open(self._filename) as csvFile:\n",
    "            f = csv.reader(csvFile)\n",
    "            self._header = next(f)\n",
    "    def getWholeCsv(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename)\n",
    "            self._isRead = True\n",
    "        return self._df\n",
    "    def getHeader(self):\n",
    "        return self._header\n",
    "    def _checkIfRead(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n",
    "            self._isRead = True\n",
    "            return False\n",
    "        return True\n",
    "    def removeWord(self,text):\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        words = text.split()\n",
    "        cleanWords = [i for i in words if i not in self._toReplace]\n",
    "        return \" \".join(cleanWords)\n",
    "    def getNextBatchCsv(self):\n",
    "        self._checkIfRead()\n",
    "        out = next(self._df,None)\n",
    "        if self._preProc and isinstance(out,pd.DataFrame):\n",
    "            out[self._postCol] = out[self._postCol].apply(self.preprocessing_v1)\n",
    "        if self._toReplace != None and isinstance(out,pd.DataFrame):\n",
    "            out[self._postCol] = out[self._postCol].apply(self.removeWord)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ counting the smallest number of data\n",
    "TEST = \"test\"\n",
    "TRAIN = \"train\"\n",
    "class Combiner:\n",
    "    '''\n",
    "    - Given multiple CsvToDf that correspond to a dataset combine them to a single dataframe\n",
    "    - return this dataframe\n",
    "    - need to return a dataframe that only has type and post as its columns\n",
    "    '''\n",
    "    def __init__(self,dataList,columnList):\n",
    "        '''\n",
    "        dataList is the CsvToDf that contains all the data and columnList is a list that contains the necessary\n",
    "        column names for a corresponding entry in dataList.\n",
    "        '''\n",
    "        assert len(dataList) == len(columnList),\"incorrect sizes for data\"\n",
    "        self._dataList = dataList\n",
    "        self._data = [None for i in range(len(dataList))]\n",
    "        self._necessaryCol = columnList\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._incrementData()\n",
    "    def getNextBatch(self):\n",
    "        '''\n",
    "        return a dataframe that contains all the aggregated data\n",
    "        '''\n",
    "        outData = pd.DataFrame(columns=[self._typeCol,self._postCol])\n",
    "        for data,colList in zip(self._data,self._necessaryCol):\n",
    "            if isinstance(data,pd.DataFrame):\n",
    "                renamedData = data[[colList[0],colList[1]]]\n",
    "                renamedData.columns = [self._typeCol,self._postCol]\n",
    "                \n",
    "                outData = outData.append(renamedData,ignore_index=True)\n",
    "        self._incrementData()\n",
    "        if (len(outData.index)) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return outData\n",
    "    def _incrementData(self):\n",
    "        for idx,i in enumerate(self._dataList):\n",
    "            self._data[idx] = i.getNextBatchCsv()\n",
    "class Balancer:\n",
    "    '''\n",
    "    - Balance the count\n",
    "    - Decide what the training and test dat will be\n",
    "    - Needs to output three data frames the train the test and the remainder\n",
    "    - make the remainder the training set\n",
    "    '''\n",
    "    def __init__(self,combiner,trainFreq,testFreq):\n",
    "        #personSize is minimum size of the number of people in a single personality group\n",
    "        self._combiner = combiner\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "        self._trainFreq = trainFreq\n",
    "        self._testFreq = testFreq\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "    def createDataSets(self):\n",
    "        self.reset()\n",
    "        while not(self._trainIsUniform()) or not(self._testIsUniform()):\n",
    "            #the three conditionals above will check if test and train dataset have uniform data \n",
    "            batch = self._combiner.getNextBatch()\n",
    "            if not(isinstance(batch,pd.DataFrame)):\n",
    "                break\n",
    "            for idx,row in batch.iterrows():\n",
    "                if isinstance(row[self._typeCol],str):\n",
    "                    personality = row[self._typeCol].upper()\n",
    "                    if personality in self._personality_count:\n",
    "                        if self._personality_count[personality][TRAIN] < self._trainFreq:\n",
    "                            self._training.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TRAIN] += 1\n",
    "                        elif self._personality_count[personality][TEST] < self._testFreq:\n",
    "                            self._testing.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TEST] += 1\n",
    "        return True\n",
    "    def reset(self):\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "    def getTrainSet(self):\n",
    "        return pd.DataFrame(self._training)\n",
    "    def getTestSet(self):\n",
    "        return pd.DataFrame(self._testing)\n",
    "    def _trainIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TRAIN] < self._trainFreq:\n",
    "                return False\n",
    "        return True\n",
    "    def _testIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TEST] < self._testFreq:\n",
    "                return False\n",
    "        return True\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPersonalityDict():\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    for idx,keys in enumerate(personality_dict):\n",
    "        oneVec = np.zeros((16,))\n",
    "        oneVec[idx] = 1\n",
    "        personality_dict[keys] = oneVec\n",
    "    return personality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterDf(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "\n",
    "    for idx,row in df.iterrows():\n",
    "        if isinstance(row[\"type\"],str):\n",
    "            personality = row[\"type\"].upper()\n",
    "            if personality in personality_dict:\n",
    "                personality_dict[personality] += 1\n",
    "    return personality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmeDcYQsH8Jd",
    "outputId": "4b3a8878-f7b5-4b10-c2bc-b3070a11dc3d"
   },
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def convertLabels(labelDf):\n",
    "    '''\n",
    "    this will turn the string labels to floats\n",
    "    '''\n",
    "    personality_dict = getPersonalityDict()\n",
    "    type_labels = []\n",
    "    # Go through the array and turn the personality type into its corresponding number\n",
    "    for idx,personality in enumerate(labelDf):\n",
    "        if isinstance(personality,str):\n",
    "            type_labels.append(personality_dict[personality.upper()])\n",
    "    return np.array(type_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def get4Dim(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    out = [[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))]]\n",
    "    for idx,row in enumerate(df):\n",
    "        personality = row\n",
    "        if isinstance(personality,str) and personality in personality_dict:\n",
    "            personality = personality.upper()\n",
    "            if personality[0] == \"E\":\n",
    "                out[0][idx] = 1\n",
    "            if personality[1] == \"S\":\n",
    "                out[1][idx] = 1\n",
    "            if personality[2] == \"T\":\n",
    "                out[2][idx] = 1\n",
    "            if personality[3] == \"J\":\n",
    "                out[3][idx] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDimModel():\n",
    "    vocab_size = 10000\n",
    "    max_length = 2016\n",
    "    embedding_dim = 256\n",
    "    return tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(64, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(64),\n",
    "                            tf.keras.layers.Dense(1, activation='sigmoid'),])\n",
    "def train4Dim(trainPost,trainLabels,num_epochs):\n",
    "    models = [createDimModel(),createDimModel(),createDimModel(),createDimModel()]\n",
    "    for idx,dims in enumerate(trainLabels):\n",
    "        print(f\"training dim {idx+1}\")\n",
    "        models[idx].compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics = [\"accuracy\"])\n",
    "        models[idx].fit(trainPost, np.array(trainLabels[idx]), epochs = num_epochs, verbose = 1)\n",
    "    return models\n",
    "def test4Dim(models,testPost,testLabels):\n",
    "    out = np.array([])\n",
    "    for idx,model in enumerate(models):\n",
    "        if len(out) == 0:\n",
    "            out = np.array([np.squeeze(model.predict(testPost))])\n",
    "        else:\n",
    "            print(out)\n",
    "            out = np.append(out,np.squeeze(model.predict(testPost)),axis=1)\n",
    "    label = np.array([[]])\n",
    "    for i in testPost:\n",
    "        if len(out) == 0:\n",
    "            label = np.array([label])\n",
    "        else:\n",
    "            label = np.append(label,i,axis=1)\n",
    "    print(f\"total accuracy for personality classification = {np.mean((np.sum(label-out,axis=1)) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWord(se,text):\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        words = text.split()\n",
    "        cleanWords = [i for i in words if i not in self._toReplace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfFactory(fileList,columns,replaceWords,personTrain):\n",
    "    '''\n",
    "    This will create a dataframe without the specified words\n",
    "    '''\n",
    "    files = []\n",
    "    for idx,i in enumerate(fileList):\n",
    "        files.append(CsvToDf(i,batchSize=400,preProc=True,postCol=columns[idx][1],toReplace=replaceWords))\n",
    "    combine = Combiner(files,columns)\n",
    "    balancer = Balancer(combine,personTrain,0)\n",
    "    balancer.createDataSets()\n",
    "    return balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "def tokenize(postsSet):\n",
    "    tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "    tokenizer.fit_on_texts(postsSet)\n",
    "    training_sequences = tokenizer.texts_to_sequences(postsSet)\n",
    "    training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = 2016)\n",
    "    # training_sequences = np.array(training_sequences)\n",
    "    training_padded = np.array(training_padded)\n",
    "    return training_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrain(fileList,columns,replaceWords,personTrain):\n",
    "    out = []\n",
    "    for i in replaceWords:\n",
    "        balancer = dfFactory(fileList,columns,i,personTrain)\n",
    "        out.append(balancer.getTrainSet())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-153-1f8764a73df4>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-153-1f8764a73df4>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print(f\"words removed: {toReplace[idx]})\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def createModels(trainSets,toReplace):\n",
    "    '''\n",
    "    trainSets must be a list of dataframes with columns posts and type\n",
    "    '''\n",
    "    out = []\n",
    "    for idx,i in enumerate(trainSets):\n",
    "        print(f\"words removed: {toReplace[idx]}\")\n",
    "        out.append(train4Dim(tokenize(i[POSTS]),get4Dim(i[TYPE]),1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels(models):\n",
    "    '''\n",
    "    models will be a list of list containing a model for each dimension and each model with some words taken out\n",
    "    '''\n",
    "    for i in models:\n",
    "        test4Dim(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesList = [\"../data/mbti_1.csv\"]\n",
    "columns = [[\"type\",\"posts\"]]\n",
    "toReplace = [[],[\"suspect\"],[\"teaching\"]]\n",
    "trainSets = createTrain(filesList,columns,toReplace,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 30s 2s/step - loss: 0.6956 - accuracy: 0.5091\n",
      "15/15 [==============================] - 30s 2s/step - loss: 0.7168 - accuracy: 0.4782\n",
      "15/15 [==============================] - 29s 2s/step - loss: 0.7005 - accuracy: 0.5121\n",
      "15/15 [==============================] - 30s 2s/step - loss: 0.6982 - accuracy: 0.5367\n",
      "15/15 [==============================] - 30s 2s/step - loss: 0.7039 - accuracy: 0.5055\n",
      "15/15 [==============================] - 29s 2s/step - loss: 0.7014 - accuracy: 0.4906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a4dea824342d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-b2b088e7681a>\u001b[0m in \u001b[0;36mcreateModels\u001b[0;34m(trainSets)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainSets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain4Dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPOSTS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget4Dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-020e8efc4e19>\u001b[0m in \u001b[0;36mtrain4Dim\u001b[0;34m(trainPost, trainLabels, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest4Dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestPost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project_scripts/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = createModels(trainSets,toReplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0][0].predict(tokenize(trainSets[0][\"posts\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(get4Dim(trainSets[0][\"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_1.csv\")\n",
    "training_size = 6675\n",
    "df = file1.getWholeCsv()\n",
    "training_labels = get4Dim(df[0:training_size][\"type\"])\n",
    "testing_labels = get4Dim(df[training_size:][\"type\"])\n",
    "testing_posts = df[training_size:][\"posts\"]\n",
    "training_posts = df[0:training_size][\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50LEptZFKXl6",
    "outputId": "566e4359-729c-4d9b-9f93-8d0e8e1f7858"
   },
   "outputs": [],
   "source": [
    "# Only considering the top 10000 most common words\n",
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "# We only want to fit the tokenizer on the training, not the testing\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\n",
    "training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# training_sequences = np.array(training_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# testing_sequences = np.array(testing_sequences)\n",
    "testing_padded = np.array(testing_padded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleLabel(labels):\n",
    "    label_copy = deepcopy(labels)\n",
    "    for idx,i in enumerate(label_copy): #this will shuffle the labels\n",
    "        np.random.shuffle(label_copy[idx])\n",
    "    return label_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutationTest(training_padded,training_labels,testing_labels,testing_padded,permutation):\n",
    "    result = []\n",
    "    for i in range(permutation):\n",
    "        label_copy = deepcopy(training_labels)\n",
    "        for idx,i in enumerate(label_copy): #this will shuffle the labels\n",
    "            np.random.shuffle(label_copy[idx])\n",
    "        models = train4Dim(training_padded,label_copy,1)\n",
    "        getAccuracy(models,testing_labels,testing_padded)\n",
    "        getTotalAccuracy(models,testing_labels,testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dim 1\n",
      "209/209 [==============================] - 319s 2s/step - loss: 0.5485 - accuracy: 0.7588\n",
      "training dim 2\n",
      "209/209 [==============================] - 325s 2s/step - loss: 0.4213 - accuracy: 0.8560\n",
      "training dim 3\n",
      "209/209 [==============================] - 316s 2s/step - loss: 0.6936 - accuracy: 0.5141\n",
      "training dim 4\n",
      "209/209 [==============================] - 328s 2s/step - loss: 0.6751 - accuracy: 0.6117\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5345\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.197\n",
      "training dim 1\n",
      "209/209 [==============================] - 316s 2s/step - loss: 0.5513 - accuracy: 0.7553\n",
      "training dim 2\n",
      "209/209 [==============================] - 315s 1s/step - loss: 0.4228 - accuracy: 0.8437\n",
      "training dim 3\n",
      "209/209 [==============================] - 316s 2s/step - loss: 0.6952 - accuracy: 0.5360\n",
      "training dim 4\n",
      "209/209 [==============================] - 319s 2s/step - loss: 0.6691 - accuracy: 0.6192\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5405\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.202\n",
      "training dim 1\n",
      "209/209 [==============================] - 445s 2s/step - loss: 0.5450 - accuracy: 0.7761\n",
      "training dim 2\n",
      "209/209 [==============================] - 443s 2s/step - loss: 0.4143 - accuracy: 0.8625\n",
      "training dim 3\n",
      "209/209 [==============================] - 416s 2s/step - loss: 0.6974 - accuracy: 0.5220\n",
      "training dim 4\n",
      "209/209 [==============================] - 408s 2s/step - loss: 0.6695 - accuracy: 0.6118\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5205\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.194\n",
      "training dim 1\n",
      "209/209 [==============================] - 455s 2s/step - loss: 0.5481 - accuracy: 0.7740\n",
      "training dim 2\n",
      "209/209 [==============================] - 453s 2s/step - loss: 0.4144 - accuracy: 0.8642\n",
      "training dim 3\n",
      "209/209 [==============================] - 455s 2s/step - loss: 0.6943 - accuracy: 0.5248\n",
      "training dim 4\n",
      "209/209 [==============================] - 446s 2s/step - loss: 0.6720 - accuracy: 0.6093\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5405\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.202\n",
      "training dim 1\n",
      "209/209 [==============================] - 452s 2s/step - loss: 0.5498 - accuracy: 0.7720\n",
      "training dim 2\n",
      "209/209 [==============================] - 460s 2s/step - loss: 0.4096 - accuracy: 0.8475\n",
      "training dim 3\n",
      "209/209 [==============================] - 456s 2s/step - loss: 0.6927 - accuracy: 0.5360\n",
      "training dim 4\n",
      "209/209 [==============================] - 457s 2s/step - loss: 0.6809 - accuracy: 0.5875\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5405\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.202\n",
      "training dim 1\n",
      "135/209 [==================>...........] - ETA: 2:35 - loss: 0.5520 - accuracy: 0.7696"
     ]
    }
   ],
   "source": [
    "permutationTest(training_padded,training_labels,testing_labels,testing_padded,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dim 1\n",
      "Epoch 1/5\n",
      "209/209 [==============================] - 434s 2s/step - loss: 0.5537 - accuracy: 0.7514\n",
      "Epoch 2/5\n",
      "209/209 [==============================] - 435s 2s/step - loss: 0.5443 - accuracy: 0.7681\n",
      "Epoch 3/5\n",
      "209/209 [==============================] - 373s 2s/step - loss: 0.5382 - accuracy: 0.7720\n",
      "Epoch 4/5\n",
      "209/209 [==============================] - 421s 2s/step - loss: 0.5395 - accuracy: 0.7715\n",
      "Epoch 5/5\n",
      "209/209 [==============================] - 342s 2s/step - loss: 0.5435 - accuracy: 0.7676\n",
      "training dim 2\n",
      "Epoch 1/5\n",
      "209/209 [==============================] - 434s 2s/step - loss: 0.4170 - accuracy: 0.8409\n",
      "Epoch 2/5\n",
      "209/209 [==============================] - 437s 2s/step - loss: 0.4077 - accuracy: 0.8599\n",
      "Epoch 3/5\n",
      "209/209 [==============================] - 436s 2s/step - loss: 0.3896 - accuracy: 0.8691\n",
      "Epoch 4/5\n",
      "209/209 [==============================] - 438s 2s/step - loss: 0.3951 - accuracy: 0.8658\n",
      "Epoch 5/5\n",
      "209/209 [==============================] - 429s 2s/step - loss: 0.3943 - accuracy: 0.8669\n",
      "training dim 3\n",
      "Epoch 1/5\n",
      "209/209 [==============================] - 438s 2s/step - loss: 0.6946 - accuracy: 0.5235\n",
      "Epoch 2/5\n",
      "209/209 [==============================] - 437s 2s/step - loss: 0.6919 - accuracy: 0.5399\n",
      "Epoch 3/5\n",
      "209/209 [==============================] - 436s 2s/step - loss: 0.6915 - accuracy: 0.5316\n",
      "Epoch 4/5\n",
      "209/209 [==============================] - 437s 2s/step - loss: 0.6900 - accuracy: 0.5433\n",
      "Epoch 5/5\n",
      "209/209 [==============================] - 307s 1s/step - loss: 0.6949 - accuracy: 0.5346\n",
      "training dim 4\n",
      "Epoch 1/5\n",
      "209/209 [==============================] - 346s 2s/step - loss: 0.6757 - accuracy: 0.6031\n",
      "Epoch 2/5\n",
      "209/209 [==============================] - 363s 2s/step - loss: 0.6734 - accuracy: 0.6043\n",
      "Epoch 3/5\n",
      "209/209 [==============================] - 346s 2s/step - loss: 0.6709 - accuracy: 0.6074\n",
      "Epoch 4/5\n",
      "209/209 [==============================] - 350s 2s/step - loss: 0.6692 - accuracy: 0.6104\n",
      "Epoch 5/5\n",
      "209/209 [==============================] - 330s 2s/step - loss: 0.6761 - accuracy: 0.5960\n"
     ]
    }
   ],
   "source": [
    "models = train4Dim(training_padded,training_labels,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(models,testing_labels,testing_padded):\n",
    "    for idx,model in enumerate(models):\n",
    "        test = testing_labels[idx]\n",
    "        modelOut = np.round(models[idx].predict(testing_padded))\n",
    "        print(f\"accuracy for dim {idx+1} personality classification = {np.mean(abs(np.squeeze(modelOut)-np.squeeze(test)) == 0)}\")\n",
    "#getAccuracy(models,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalAccuracy(models,testing_labels,testing_padded):\n",
    "    total = None\n",
    "    isEmpty = True\n",
    "    for idx,model in enumerate(models):\n",
    "        test = testing_labels[idx]\n",
    "        modelOut = np.squeeze(np.round(models[idx].predict(testing_padded)))\n",
    "        if isEmpty:\n",
    "            total = np.array(modelOut)\n",
    "            isEmpty = False\n",
    "        else:\n",
    "            total = np.column_stack((total,modelOut))\n",
    "    labels = None\n",
    "    isEmpty = True\n",
    "    for idx,col in enumerate(testing_labels):\n",
    "        if isEmpty:\n",
    "            labels = np.array(col)\n",
    "            isEmpty = False\n",
    "        else:\n",
    "            labels = np.column_stack((labels,col))\n",
    "    print(f\"total accuracy for personality classification = {np.mean(np.sum(abs(total-labels),axis=1) == 0)}\")\n",
    "#getTotalAccuracy(models,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4Dim(models,testing_padded,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AfPpLGS7MunO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# ^^ actually i dont know if that is true\n",
    "embedding_dim = 256\n",
    "'''\n",
    "Embedding layer will always have vocab_size*embedding_dim parameters. Since vocab_size is 10,000 the number of parameters on this layer will always be large\n",
    "'''\n",
    "model = tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(256, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(128),\n",
    "                            tf.keras.layers.Dense(16, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = 'sgd', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6wi-9cvRtM6",
    "outputId": "30be599c-d625-4336-af86-7462bd9b982f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2016, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 2016, 256)         393984    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 3,005,328\n",
      "Trainable params: 3,005,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs = num_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      " 128/1600 [=>............................] - ETA: 3:03 - loss: 2.8088 - acc: 0.0547"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpMLBWYmzYM5"
   },
   "outputs": [],
   "source": [
    "res = np.argmax(model.predict(testing_padded),axis=1)\n",
    "label = np.argmax(testing_labels,axis=1)\n",
    "print(f\"accuracy = {np.mean((label-res) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mbti_classifier_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
