{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"/kaggle/input/mbti-type/mbti_1.csv\", \"r\")\n",
    "#f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a0a28828f106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHashingVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "\n",
    "'''\n",
    "I used a slightly modified version of NaiveBayes classifier from AS1 here\n",
    "\n",
    "'''\n",
    "\n",
    "class MyBayesClassifier():\n",
    "    def __init__(self, smooth=1):\n",
    "        self._smooth = smooth # This is for additive smoothing\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        alpha_smooth = self._smooth\n",
    "        cls = np.unique(y)\n",
    "        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n",
    "\n",
    "        self._cls = cls\n",
    "        self._prior = np.zeros((1,Ncls))\n",
    "        #initialize a matrix stand for all p(x|y),the likelyhood for every attribute\n",
    "        self._likehood = np.zeros((Ncls,Nfeat))\n",
    "        #for each class,find rows that satisfies the condition,and compute the likehood of each feature\n",
    "        for i in range(Ncls):\n",
    "            cla = cls[i]\n",
    "            x_cla = X[y==cla]                                              #the rows that belong to current class\n",
    "            self._prior[0,i] =(x_cla.shape[0]+alpha_smooth)/(X.shape[0]+alpha_smooth*Ncls)                    #compute prior probability of current class\n",
    "            #self._likehood[i] = np.sum(x_cla,axis=0)/x_cla.shape[0]       #verticlly summation along each column to get frequency of each feature and then divide by # of rows\n",
    "\n",
    "            self._likehood[i,:] = (np.sum(x_cla, axis=0)+alpha_smooth) / (x_cla.shape[0]+ alpha_smooth * 2)                  #apply smooth to frequency divide # of rows\n",
    "            #print(self._likehood)\n",
    "            #print(self._prior)\n",
    "\n",
    "    def train_JM_smooth(self, X, y,X_dataset):\n",
    "        alpha_smooth = self._smooth\n",
    "        cls = np.unique(y)\n",
    "        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n",
    "\n",
    "        size_dataset = X_dataset.shape[0]             #number of data in dataset\n",
    "        feature_dataset = np.sum(X_dataset,axis=0)    #count the appearnce of each word in dataset by verticlly sum along each column from the dataset\n",
    "\n",
    "\n",
    "        self._cls = cls\n",
    "        self._prior = np.zeros((1,Ncls))\n",
    "        #initialize a matrix stand for all p(x|y),the likelyhood for every attribute\n",
    "        self._likehood = np.zeros((Ncls,Nfeat))\n",
    "        #for each class,find rows that satisfies the condition,and compute the likehood of each feature\n",
    "        for i in range(Ncls):\n",
    "            cla = cls[i]\n",
    "            x_cla = X[y==cla]                                              #the rows that belong to current class\n",
    "            self._prior[0,i] =x_cla.shape[0]/X.shape[0]                    #compute prior probability of current class\n",
    "            #self._likehood[i] = np.sum(x_cla,axis=0)/x_cla.shape[0]       #verticlly summation along each column to get frequency of each feature and then divide by # of rows\n",
    "            numerator1 = np.sum(x_cla, axis=0)\n",
    "            denominator1 = np.sum(x_cla)\n",
    "\n",
    "            self._likehood[i,:] = (1- alpha_smooth)*numerator1/denominator1 + alpha_smooth * feature_dataset / size_dataset                  #apply JM smooth to frequency divide # of rows\n",
    "\n",
    "        #after the for loop,self._prior stores the prior probability of all catergories and\n",
    "        #selef._likehood stores the probabality of all feature  such that  P(feature_i | catergory)\n",
    "            ###confusion  : smoothing????????\n",
    "        #self._notlikehood = 1-self._likehood              #P(not feature i | catergory )\n",
    "    def predict(self, X):\n",
    "\n",
    "        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]                         #number of test sample\n",
    "        pred = np.zeros(Ntest)\n",
    "        loglikehood = np.log(self._likehood)       \n",
    "        X_not = 1-X                   #since in original data,1 for appearence of feature i and 0 for not appear,the logic not means 1 for not appear and 0 for appear\n",
    "\n",
    "        #X is size Ntest x Nfeat,selef._likehood.T is the shape Nfeat x Ncls\n",
    "        log_appear = X.dot(np.log(self._likehood.T))\n",
    "        \n",
    "        #print(\"appear\",log_appear)\n",
    "        \n",
    "        log_absence = X_not.dot (np.log(1-self._likehood.T))\n",
    "        #print(\"absence\",log_absence)\n",
    "\n",
    "        log_post = log_appear+log_absence\n",
    "        #log_post = X.dot(np.log(self._likehood.T)) + X_not.dot (np.log(1-self._likehood.T))    #consider both appearence and absence\n",
    "        #log_post = X.dot(np.log(self._likehood.T))                                             #only consider appearence\n",
    "\n",
    "        log_post = log_post + np.log(self._prior.reshape(1,Ncls))\n",
    "        #print(log_post)\n",
    "        pred = self._cls[np.argmax(log_post,axis=1)]\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/mbti-type/mbti_1.csv',header=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(50, 20)\n",
    "sns.catplot(x=\"type\", kind=\"count\", data=df,height=8.27, aspect=11.7/8.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Just striping the string incase of any whitespace before or after the string\n",
    "df[\"type\"] = df[\"type\"].str.strip()\n",
    "# Seperate the the label into four different parts\n",
    "target_multi_label = df[\"type\"].str.split(\"\" , expand=True)\n",
    "target_multi_label = target_multi_label.iloc[: , 1:-1]\n",
    "target_multi_label.columns = [\"Personality-1\",\"Personality-2\",\"Personality-3\",\"Personality-4\"]\n",
    "\n",
    "df = pd.concat([df,target_multi_label] , axis=1)\n",
    "'''\n",
    "personality_map = {\n",
    "    \"I\":\"Introvert\",\n",
    "    \"E\":\"Extrovert\",\n",
    "    \"N\":\"Intuitive\",\n",
    "    \"S\":\"Sensitive\",\n",
    "    \"F\":\"Emotional\",\n",
    "    \"T\":\"Thinker\",\n",
    "    \"J\":\"Judgemental\",\n",
    "    \"P\":\"Perceiving\"\n",
    "}\n",
    "for col in df.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n",
    "    df[col] = df[col].map(personality_map)\n",
    "'''\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(50, 20)\n",
    "sns.catplot(x=\"Personality-1\", kind=\"count\", data=df,height=5, aspect=4/5)\n",
    "sns.catplot(x=\"Personality-2\", kind=\"count\", data=df,height=5, aspect=4/5)\n",
    "sns.catplot(x=\"Personality-3\", kind=\"count\", data=df,height=5, aspect=4/5)\n",
    "sns.catplot(x=\"Personality-4\", kind=\"count\", data=df,height=5, aspect=4/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version1 of text pre-processing\n",
    "\n",
    "#source:https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "!pip install Unidecode\n",
    "!pip install contractions\n",
    "!pip install BeautifulSoup4\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode \n",
    "#from word2number import w2n\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "def preprocessing_v1(text):\n",
    "    #remove html information\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    processed = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove http// \n",
    "    processed = re.sub(r\"http\\S+\", \"\", processed)\n",
    "\n",
    "    #remove ||| seperate\n",
    "    processed = re.sub(r'\\|\\|\\|', r' ', processed)\n",
    "\n",
    "    #lower case\n",
    "    processed = processed.lower()\n",
    "\n",
    "    #expand shortened words, e.g. don't to do not\n",
    "    processed = contractions.fix(processed)\n",
    "\n",
    "    #remove accented char\n",
    "    processed = unidecode.unidecode(processed)\n",
    "\n",
    "    #remove white space\n",
    "    #processed = processed.strip()\n",
    "    #processed = \" \".join(processed.split())\n",
    "\n",
    "    # Lemmatizing \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    processed=lemmatizer.lemmatize(processed)\n",
    "\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posts'] = df['posts'].apply(preprocessing_v1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split traning and test data and vectorize them\n",
    "\n",
    "\n",
    "\n",
    "number_training = 6000\n",
    "data_size = df['type'].shape[0]\n",
    "\n",
    "\n",
    "\n",
    "all_data = df['posts'].astype('U').values\n",
    "data_train = df['posts'][:number_training].astype('U').values\n",
    "data_test = df['posts'][number_training:].astype('U').values\n",
    "\n",
    "y_train = df['type'][:number_training].astype('U').values\n",
    "y_test = df['type'][number_training:].astype('U').values\n",
    "\n",
    "\n",
    "#Note here, increase max_features may result in increasing ram usage and cause crush of colab\n",
    "#By defaut,it will geneate over 140000 features without any text preprocessing,it would decrease to near 100000 but still not acceptable\n",
    "#therefore I added a upper bound for max_features\n",
    "vectorizer = CountVectorizer(\n",
    "        lowercase=True, stop_words='english',\n",
    "        max_df=1.0, min_df=1, max_features=2000,  binary=True\n",
    "      )\n",
    "processed_data = vectorizer.fit_transform(all_data).toarray()\n",
    "\n",
    "X_train = processed_data[0:number_training, :]\n",
    "X_test = processed_data[number_training:, :]\n",
    "\n",
    "print(\"X_train.shape = {}\".format(X_train.shape))\n",
    "print(\"X_test.shape = {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   tempoary test cell , only used to debug some non-sense\n",
    "print(X_train)\n",
    "print(sum(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform naive bayes,predict 1 among 16 personality types at once\n",
    "clf = MyBayesClassifier(1.0)\n",
    "clf.train(X_train, y_train);\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Absolute accuracy = {}\".format(np.mean(y_test==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each sub-personality type,train the model and make prediction\n",
    "#first test I/E,then N/S.......Cancadinate the result together to form final result\n",
    "y_pred = a2 = np.array(['' for i in range(data_size-number_training)])\n",
    "print(y_pred.shape)\n",
    "print(type(y_pred))\n",
    "clf = MyBayesClassifier(1.0)\n",
    "\n",
    "for col in df.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n",
    "    y_train_sub = df[col][:6000].astype('U').values\n",
    "    clf.train(X_train, y_train_sub);\n",
    "    y_pred_sub = clf.predict(X_test)\n",
    "\n",
    "    y_pred=np.core.defchararray.add(y_pred, y_pred_sub)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(\"Absolute accuracy = {}\".format(np.mean(y_test==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "'''\n",
    "This will load the csv\n",
    "'''\n",
    "class CsvToDf:\n",
    "    '''\n",
    "    This class will simply turn the given data to a dataframe\n",
    "    '''\n",
    "    def __init__(self,filename,batchSize=None,cols=None):\n",
    "        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n",
    "        #into memory\n",
    "        self._cols = cols\n",
    "        self._header = None\n",
    "        self._filename = filename\n",
    "        self._curIndex = 0     #this will be the current index that we are in the csv\n",
    "        self._isRead = False\n",
    "        self._df = None\n",
    "        self._storeHeader()\n",
    "        self._batchSize = batchSize\n",
    "    def _storeHeader(self):\n",
    "        with open(self._filename) as csvFile:\n",
    "            f = csv.reader(csvFile)\n",
    "            self._header = next(f)\n",
    "    def getWholeCsv(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename)\n",
    "            self._isRead = True\n",
    "        return self._df\n",
    "    def getHeader(self):\n",
    "        return self._header\n",
    "    def _checkIfRead(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n",
    "            self._isRead = True\n",
    "            return False\n",
    "        return True\n",
    "    def getNextBatchCsv(self):\n",
    "        self._checkIfRead()\n",
    "        return next(self._df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one is the early building version of LOOCV method\n",
    "def LOOCV(k):\n",
    "    all_data = df['posts'].astype('U').values\n",
    "    #use int() to eliminate decimals\n",
    "    data_fragment_size = int(all_data / k)\n",
    "    #we vectorize the data first, but due to the ram overload, it is still an issue to be resolved\n",
    "    #for now, let's keep the max_feature limit\n",
    "    #the all_data should have the size of 2000 for now\n",
    "    vectorizer = CountVectorizer(\n",
    "        lowercase=True, stop_words=None,\n",
    "        max_df=1.0, min_df=1, max_features=2000,  binary=True\n",
    "    )\n",
    "    processed_data = vectorizer.fit_transform(all_data).toarray()\n",
    "\n",
    "    for i in range(0, k):\n",
    "        lower_bound = i * data_fragment_size\n",
    "        upper_bound = lower_bound + data_fragment_size\n",
    "        #split the data into training and testing based on k\n",
    "        #this part is just the modified version of the normal test part written by Zepeng Xiao\n",
    "\n",
    "        data_train = df['posts'][:lower_bound].astype('U').values + df['posts'][upper_bound:].astype('U').values\n",
    "        y_train = df['type'][:lower_bound].astype('U').values + df['posts'][upper_bound:].astype('U').values\n",
    "\n",
    "        data_test = df['posts'][lower_bound:upper_bound].astype('U').values\n",
    "        y_test = df['type'][lower_bound:upper_bound].astype('U').values\n",
    "\n",
    "        #not sure why we would both have data_train and x_train\n",
    "        #but I'll keep it like that anyway\n",
    "        x_train = processed_data[:lower_bound, :] + processed_data[upper_bound:, :]\n",
    "        x_test = processed_data[lower_bound:upper_bound, :]\n",
    "\n",
    "        #feed the data to the model and get the results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test for small Dset\n",
    "'''\n",
    "test = CsvToDf(\"../input/mbti-type/mbti_1.csv\",batchSize=200)\n",
    "print(test.getNextBatchCsv())\n",
    "res = test.getWholeCsv()\n",
    "print(type(res))\n",
    "print(test.getWholeCsv())\n",
    "print(test.getHeader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                  body  subreddit\n",
      "100  I m really really hot. Like walking heater kin...       intj\n",
      "101  I'm going to throw some advice your way. You'r...       infj\n",
      "102                                               lmao       infp\n",
      "103  Sensible doesn't have to mean strict and borin...       INTP\n",
      "104  But these are some insane projections. They ta...     europe\n",
      "..                                                 ...        ...\n",
      "195  While i wrote my own feelings in a reply, i al...       infj\n",
      "196  Usually my dreams are neutral, but I do get go...       ENFP\n",
      "197                                       I dig that.   introvert\n",
      "198  It really depends on how mature she is. \\n\\n\\n...       enfj\n",
      "199  She's 19. She was there. She's an ESFP. She wa...       intj\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "                                                  body  subreddit\n",
      "200                                  You're only human       mbti\n",
      "201  I'm in the middle of this right now. This thre...  introvert\n",
      "202                         No, why would I say that?        infp\n",
      "203                           Super sensitive (aux Fi)       mbti\n",
      "204  I don't have a blog, blog. I have a tumblr..it...       INTP\n",
      "..                                                 ...        ...\n",
      "295           Congratulations to you and your dad too!       infj\n",
      "296  The first paragraph, while true for real estat...       intj\n",
      "297  Though I'm not from Belgium, I am gonna send t...       entp\n",
      "298  We're like the most obese country in Europe, man.     europe\n",
      "299  Yes! I am a head cashier at B&amp;N and part o...       infp\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "['author_flair_text', 'body', 'subreddit']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test2 = CsvToDf(\"../data/mbti_full_pull.csv\",batchSize=100,cols=['body', 'subreddit'])\n",
    "#test2.eliminateCols(['created_utc', 'subreddit', 'author', 'domain', 'url', 'num_comments', 'score', 'ups', 'downs', 'selftext', 'saved', 'id', 'from_kind', 'gilded', 'from', 'stickied', 'retrieved_on', 'over_18', 'thumbnail', 'subreddit_id', 'hide_score', 'link_flair_css_class', 'author_flair_css_class', 'archived', 'is_self', 'from_id', 'permalink', 'name', 'author_flair_text', 'quarantine', 'link_flair_text', 'distinguished'])\n",
    "print(type(test2.getNextBatchCsv()))\n",
    "print(test2.getNextBatchCsv())\n",
    "print(test2.getNextBatchCsv())\n",
    "print(test2.getHeader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatData(data,label,trainSize):\n",
    "        vectorizer = HashingVectorizer(\n",
    "        lowercase=True, stop_words='english',binary=True)\n",
    "        out_data = vectorizer.fit_transform(data.astype('U').values).toarray()\n",
    "        out_label = label.str.lower().astype('U').values\n",
    "        return (out_data[:trainSize],out_data[trainSize:],out_label[:trainSize],out_label[trainSize:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1048576)\n",
      "(100, 1048576)\n",
      "(100, 1048576)\n",
      "(100, 1048576)\n"
     ]
    }
   ],
   "source": [
    "formatter = Formatter()\n",
    "test = CsvToDf(\"../data/mbti_full_pull.csv\",batchSize=100,cols=['body', 'subreddit'])\n",
    "#test2.eliminateCols(['created_utc', 'subreddit', 'author', 'domain', 'url', 'num_comments', 'score', 'ups', 'downs', 'selftext', 'saved', 'id', 'from_kind', 'gilded', 'from', 'stickied', 'retrieved_on', 'over_18', 'thumbnail', 'subreddit_id', 'hide_score', 'link_flair_css_class', 'author_flair_css_class', 'archived', 'is_self', 'from_id', 'permalink', 'name', 'author_flair_text', 'quarantine', 'link_flair_text', 'distinguished'])\n",
    "data = test.getNextBatchCsv()\n",
    "xTrain,xTest,yTrain,yTest = formatter.formatData(data['body'],data['subreddit'],100)\n",
    "print(xTrain.shape)\n",
    "data = test.getNextBatchCsv()\n",
    "xTrain,xTest,yTrain,yTest = formatter.formatData(data['body'],data['subreddit'],100)\n",
    "print(xTrain.shape)\n",
    "data = test.getNextBatchCsv()\n",
    "xTrain,xTest,yTrain,yTest = formatter.formatData(data['body'],data['subreddit'],100)\n",
    "print(xTrain.shape)\n",
    "data = test.getNextBatchCsv()\n",
    "xTrain,xTest,yTrain,yTest = formatter.formatData(data['body'],data['subreddit'],100)\n",
    "print(xTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NB classifier that can train on batches\n",
    "'''\n",
    "class BayesClassifier_smooth():\n",
    "    def __init__(self,smooth=1):\n",
    "        self._smooth = smooth # This is for additive smoothing\n",
    "        self._cls = ['estj', 'estp', 'esfj', 'esfp', 'entj', 'entp', 'enfj', 'enfp', 'istj', 'istp', 'isfj', 'isfp', 'intj', 'intp', 'infj', 'infp']\n",
    "        self._prior = np.zeros((16,1))                        #initialize a (Ncl * 1) shape matrix to store the count of each label,used to calculate prior probability later\n",
    "        self._likehood = None\n",
    "        self._notInit = True\n",
    "        self._prob_matrix = None\n",
    "        #given test dataset we need to reshape likelihood to size of test data\n",
    "    def update_smooth(i):\n",
    "        self._smooth = i\n",
    "        \n",
    "    #the train method would only count the probability now\n",
    "    def train(self, X, y):\n",
    "        cls = np.unique(y)\n",
    "        Nfeat = X.shape[1] #Nfeat: number of features.\n",
    "        if self._notInit:\n",
    "            self._likehood = np.zeros((16,Nfeat))\n",
    "            self._notInit = False\n",
    "        for i in range(16):\n",
    "            cla = self._cls[i]                                                #cla <---- current class\n",
    "            x_cla = X[y==cla]                                              #a subset of the rows that belong to current class\n",
    "            self._prior[i,0] += x_cla.shape[0]                               #count frequency of current class \n",
    "            #verticlly summation along each column to get frequency of each feature appear given current class\n",
    "            self._likehood[i,:] += np.sum(x_cla, axis=0)\n",
    "\n",
    "\n",
    "    #this method used for report the predict when using JM smooth approach for pard(d)\n",
    "    #parameter X is the training data,X_dataset is also used here to calculate the feature appreance count in whole dataset to avoid zero probability\n",
    "    def _compute_prediction(self,data_point):\n",
    "        '''\n",
    "        precondition: datapoint must be a list of integers that are either 0 or 1. And its length must be the same as the number of features.\n",
    "        postcondition: no side effects\n",
    "        this will return an array of 16 floats [0,1] each corresponding to one of the possible personality types\n",
    "        '''\n",
    "        data_point_matrix = np.array([data_point,]*self._prob_matrix.shape[0])\n",
    "        true_matrix = data_point_matrix * self._prob_matrix\n",
    "        false_matrix = ((data_point_matrix+1)%2) * (1-self._prob_matrix)\n",
    "        label_array = np.squeeze(np.sum(np.log(true_matrix+false_matrix),axis=1))\n",
    "        prior_array = np.squeeze(np.log((self._prior+self._smooth)/(16*self._smooth+sum(self._prior))))\n",
    "        return np.squeeze(label_array+prior_array)\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function has to return a numpy array of shape X.shape[0] (i.e. of shape \"number of testing examples\")\n",
    "        '''\n",
    "        self._prob_matrix = (self._likehood+self._smooth)/(self._prior + self._smooth*2)\n",
    "        #For each row in X determine the likelihood of it being one of the personalities\n",
    "        result = np.apply_along_axis(self._compute_prediction,axis=1,arr=X)\n",
    "        #result will be a matrix that has the shape of (# rows in X,# of possible personalities).\n",
    "        #each entry will represent the probability of a data point being a specific personality\n",
    "        pred = np.argmax(result,axis=1)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strLabelToInt(inpLabel):\n",
    "    out = np.zeros((inpLabel.shape))\n",
    "    labels = ['estj', 'estp', 'esfj', 'esfp', 'entj', 'entp', 'enfj', 'enfp', 'istj', 'istp', 'isfj', 'isfp', 'intj', 'intp', 'infj', 'infp']\n",
    "    for idx,i in enumerate(inpLabel):\n",
    "        if i.lower() in labels:\n",
    "            out[idx] = labels.index(i)\n",
    "        else:\n",
    "            out[idx] = -100\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.14\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Naive Bayes Test 1\n",
    "'''\n",
    "classifier = BayesClassifier_smooth()\n",
    "test = CsvToDf(\"../data/mbti_full_pull.csv\",batchSize=200,cols=['body', 'subreddit'])\n",
    "data = test.getNextBatchCsv()\n",
    "xTrain,xTest,yTrain,yTest = formatData(data['body'],data['subreddit'],100)\n",
    "yTest = strLabelToInt(yTest)\n",
    "classifier.train(xTrain,yTrain)\n",
    "pred = classifier.predict(xTest)\n",
    "print(\"accuracy = {}\".format(np.mean((yTest-pred)==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "format data test\n",
    "'''\n",
    "test = CsvToDf(\"../data/mbti_full_pull.csv\",cols=['body','subreddit'],batchSize=200)\n",
    "data = test.getNextBatchCsv()\n",
    "xTrain,xTest,yTrain,yTest = formatData(data['body'],data['subreddit'],100)\n",
    "print(xTrain)\n",
    "print(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Zepeng Xiao Version\n",
    "###In this version,we only need train data once to capture needed probability\n",
    "#could change smooth factor by cls._smooth = smoothfactor or update_smooth and call predict method\n",
    "####\n",
    "class BayesClassifier_smooth():\n",
    "    def __init__(self, smooth=1):\n",
    "        self._smooth = smooth # This is for additive smoothing\n",
    "    \n",
    "    def update_smooth(i):\n",
    "        self._smooth = i\n",
    "    \n",
    "    #the train method would only count the probability now\n",
    "    def train(self, X, y):\n",
    "        alpha_smooth = self._smooth\n",
    "        cls = np.unique(y)\n",
    "        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n",
    "\n",
    "        self._train_size = X.shape[0]                           #store the number of training data\n",
    "        self._feature_count = np.sum(X,axis=0)                  #count the total appear time of all features(words) by vertically summation every column of training set\n",
    "        self._cls = cls                                         #store classes for predict use\n",
    "        self._prior = np.zeros((Ncls,1))                        #initialize a (Ncl * 1) shape matrix to store the count of each label,used to calculate prior probability later\n",
    "        self._word_count = np.zeros((Ncls,1))                   #initialize the (Ncl *1) matrix to store the total count of appearence of each word given class \n",
    "\n",
    "        self._likehood = np.zeros((Ncls,Nfeat))                 #initialize to store for all the count of each word given class\n",
    "                                                                #the number of rows equal to number of classes,column number equal to number of features(words)\n",
    "                                                                #therefore it will be used to calculate vectorlized likelyhood p(x|y) later\n",
    "\n",
    "        #for each class,find rows that satisfies the condition,and capture the count from training set so that it can be used to calculate probability\n",
    "        for i in range(Ncls):\n",
    "            cla = cls[i]                                                   #cla <---- current class\n",
    "            x_cla = X[y==cla]                                              #a subset of the rows that belong to current class\n",
    "            self._prior[i,0] =x_cla.shape[0]                               #count frequency of current class \n",
    "\n",
    "            #verticlly summation along each column to get frequency of each feature appear given current class\n",
    "            self._likehood[i,:] = np.sum(x_cla, axis=0)\n",
    "\n",
    "            #sum through the subset training data of current class,count the total number of appearence of all words,used for JMM smoothing later\n",
    "            self._word_count[i,:] = np.sum(x_cla)\n",
    "\n",
    "\n",
    "    #this method used for report the predict when using JM smooth approach for pard(d)\n",
    "    #parameter X is the training data,X_dataset is also used here to calculate the feature appreance count in whole dataset to avoid zero probability\n",
    "    def predict_JM(self,X,X_dataset):                       \n",
    "        alpha_smooth = self._smooth                               #the smooth parameter alpha\n",
    "        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]   #Ncls:number of class,Ntest:Number of test data,Nfeat:number of features\n",
    "        pred = np.zeros(Ntest)                                    #initialized the numpy array of predicted result,its size equals to number of test data\n",
    "        X_not = np.logical_not(X)                                 #for original data,1 for appearence of feature i and 0 for not appear,its logic not means 1 for not appear and 0 for appear\n",
    "\n",
    "                                                      #prior and likelyhood(probability) after additive smoothing\n",
    "        size_dataset = X_dataset.shape[0]             #number of data in dataset\n",
    "        feature_dataset = np.sum(X_dataset,axis=0)    #count the appearnce of each word in dataset by verticlly sum along each column from the dataset\n",
    "\n",
    "        #calculte prior probability by divide the count of class in training data by the training size\n",
    "        prior = self._prior / self._train_size\n",
    "\n",
    "        #compute the likelyhood probabity such that p(word i exist | class y) using JM smooth\n",
    "        likelyhood = (1-alpha_smooth)*(self._likehood/self._word_count) + (alpha_smooth) * (feature_dataset / size_dataset)\n",
    "        \n",
    "        #apply log transformation of likelyhood P(xi exists | y) and P(xi not exists | y)\n",
    "        not_likelyhood = np.log(1-likelyhood)\n",
    "        likelyhood = np.log(likelyhood)\n",
    "\n",
    "        #apply dot product to obtain the summation of log(P(xi exists | y)) and P(xi not exists | y)\n",
    "        log_appear = np.dot(X,likelyhood.T)\n",
    "        log_absence = np.dot (X_not,not_likelyhood.T)\n",
    "\n",
    "        #calculate log tranformed posterior probability\n",
    "        log_post = log_appear+log_absence + np.log(prior.reshape(1,Ncls))\n",
    "\n",
    "        #choose the y such that maximum the postior from the and return it\n",
    "        pred = np.argmax(log_post,axis=1)\n",
    "\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        This function has to return a numpy array of shape X.shape[0] (i.e. of shape \"number of testing examples\")\n",
    "        '''\n",
    "\n",
    "        alpha_smooth = self._smooth                                              #the smooth parameter alpha\n",
    "        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]                  #Ncls:number of class,Ntest:Number of test data,Nfeat:number of features\n",
    "        pred = np.zeros(Ntest)                                                   #initialized the numpy array of predicted result,its size equals to number of test data\n",
    "        X_not = np.logical_not(X)                   #since in original data,1 for appearence of feature i and 0 for not appear,the logic not will give us the 1 for word not appear,and 0 for appear \n",
    "                                                    #used for vectorlizd multiplication(dot product) to get posterior\n",
    "        #calculte prior probability by divide the count of class in training data by the training size\n",
    "        prior = self._prior / self._train_size\n",
    "\n",
    "\n",
    "        #compute the likelyhood probabity such that p(word i exist | class y) using additive smooth\n",
    "        likelyhood = (self._likehood+alpha_smooth)/(self._prior + alpha_smooth*2)      #add alpha to numeriter and 2*alpha(cases of appear or not) to denominator\n",
    "\n",
    "\n",
    "        #X is in size Ntest x Nfeat, log(likehood.T) is the shape Nfeat x Ncls, each feature has 2 column in this case,each record its likelyhood of given y\n",
    "        #their product in shape Ntest x Ncls,for each test data,it has the summation of log(P(Xi exist | yi))\n",
    "      \n",
    "        log_appear = X.dot(np.log(likelyhood.T))\n",
    "\n",
    "        #1-likelyhood.T would generate all P(word i does not exist|class y),same size as likelyhood.T\n",
    "        #X_not dot product the log tranformation likelyhood for not appearence is the summation of log(P(xi does not exist | yi)) \n",
    "        log_absence = X_not.dot (np.log(1-likelyhood.T))\n",
    "\n",
    "        #add them up,it would equal the log transformed likelyhood used for naive bayes\n",
    "        log_post = log_appear+log_absence\n",
    "        \n",
    "        #add the log transformed prior probability,it become log transformed posterior probability\n",
    "        log_post = log_post + np.log(prior.reshape(1,Ncls))\n",
    "\n",
    "        #choose the y such that maximum the postior from the and return it\n",
    "        pred = self._cls[np.argmax(log_post,axis=1)]\n",
    "\n",
    "\n",
    "        return pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
