{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "6BTTDiOOd_5J",
    "outputId": "8f38426d-663a-48f8-9e2f-8f33668a53ad"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f1c93d433c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ls '/content/drive'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!ls '/content/drive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJYuGsKFeGUz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "\n",
    "filePath = \"../data/\"\n",
    "\n",
    "df = pd.read_csv(filePath,header=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxrOhLfzj0G7"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(50, 20)\n",
    "sns.catplot(x=\"type\", kind=\"count\", data=df,height=8.27, aspect=11.7/8.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dzztkktkFPF"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Just striping the string incase of any whitespace before or after the string\n",
    "df[\"type\"] = df[\"type\"].str.strip()\n",
    "# Seperate the the label into four different parts\n",
    "target_multi_label = df[\"type\"].str.split(\"\" , expand=True)\n",
    "target_multi_label = target_multi_label.iloc[: , 1:-1]\n",
    "target_multi_label.columns = [\"Personality-1\",\"Personality-2\",\"Personality-3\",\"Personality-4\"]\n",
    "\n",
    "df = pd.concat([df,target_multi_label] , axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLTe2qMNkLzT"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(50, 20)\n",
    "sns.catplot(x=\"Personality-1\", kind=\"count\", data=df,height=5, aspect=4/5)\n",
    "sns.catplot(x=\"Personality-2\", kind=\"count\", data=df,height=5, aspect=4/5)\n",
    "sns.catplot(x=\"Personality-3\", kind=\"count\", data=df,height=5, aspect=4/5)\n",
    "sns.catplot(x=\"Personality-4\", kind=\"count\", data=df,height=5, aspect=4/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48CoWsEQkOXv"
   },
   "outputs": [],
   "source": [
    "#version1 of text pre-processing\n",
    "\n",
    "#source:https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "!pip install Unidecode\n",
    "!pip install contractions\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode \n",
    "#from word2number import w2n\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "def preprocessing_v1(text):\n",
    "    #remove html information\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    processed = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove http// \n",
    "    processed = re.sub(r\"http\\S+\", \"\", processed)\n",
    "\n",
    "    #remove ||| seperate\n",
    "    processed = re.sub(r'\\|\\|\\|', r' ', processed)\n",
    "\n",
    "    #lower case\n",
    "    processed = processed.lower()\n",
    "\n",
    "    #expand shortened words, e.g. don't to do not\n",
    "    processed = contractions.fix(processed)\n",
    "\n",
    "    #remove accented char\n",
    "    processed = unidecode.unidecode(processed)\n",
    "\n",
    "    #remove white space\n",
    "    #processed = processed.strip()\n",
    "    #processed = \" \".join(processed.split())\n",
    "\n",
    "    # Lemmatizing \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    processed=lemmatizer.lemmatize(processed)\n",
    "\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr6aWGq7kWzg"
   },
   "outputs": [],
   "source": [
    "df['posts'] = df['posts'].apply(preprocessing_v1)\n",
    "posts = df['posts']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j_Dkn7lkoFT"
   },
   "outputs": [],
   "source": [
    "posts = df['posts']\n",
    "for i in range(10):\n",
    "    print(posts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaDdROHiS2YA"
   },
   "outputs": [],
   "source": [
    "# Labels for introversion vs extraversion\n",
    "first_dimension = df['Personality-1']\n",
    "# The model can't predict a float from a char, so we replace the I with 0 and E with 1\n",
    "# I couldn't do this in place for some reason so i made a new array. This runtime is pretty shit cuz of append\n",
    "\n",
    "# Make this np.zeros_lke first dim to fix runtime (?)\n",
    "first_dim = []\n",
    "\n",
    "for idx, letter in enumerate(first_dimension):\n",
    "    if letter is 'I':\n",
    "        first_dim.append(0)\n",
    "    else:\n",
    "        first_dim.append(1)\n",
    "\n",
    "first_dim = np.array(first_dim)\n",
    "print(first_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmeDcYQsH8Jd"
   },
   "outputs": [],
   "source": [
    "full_type_labels = df['type']\n",
    "# Enumerating the personality types so that our model can work with numbers\n",
    "personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 1, \"ENTP\" : 2, \"INTP\" : 3, \"INFJ\" : 4, \"INFP\" : 5, \"ENFJ\" : 6 , \n",
    "                    \"ENFP\" : 7, \"ESTP\" : 8, \"ESTJ\" : 9, \"ISTP\" : 10, \"ISTJ\" : 11, \"ISFJ\" : 12, \"ISFP\" : 13, \n",
    "                    \"ESFJ\" : 14, \"ESFP\" : 15}\n",
    "\n",
    "type_labels = []\n",
    "\n",
    "# Go through the array and turn the personality type into its corresponding number\n",
    "for idx, personality in enumerate(full_type_labels):\n",
    "    type_labels.append(personality_dict[personality])\n",
    "\n",
    "type_labels = np.array(type_labels)\n",
    "\n",
    "print(type_labels)\n",
    "# print(full_type_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ksc7-r5nDNh"
   },
   "outputs": [],
   "source": [
    "###\n",
    "#   Commenting out this whole block for now because it only predicts one letter\n",
    "#\n",
    "####\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Only considering the top 10000 most common words\n",
    "# vocab_size = 10000\n",
    "# max_length = 2016\n",
    "# # We have 8675 rows\n",
    "# training_size = 8675//2\n",
    "# training_posts = posts[0:training_size]\n",
    "# testing_posts = posts[training_size:]\n",
    "# # Right now is only predicting introversion or extroversion\n",
    "# training_labels = first_dim[0:training_size]\n",
    "# testing_labels = first_dim[training_size:]\n",
    "\n",
    "# # We only want to fit the tokenizer on the training, not the testing\n",
    "# tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "# tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "# # Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# # The longest post in our dataset is 2016, but we should truncate='post earlier than 2016 words\n",
    "# training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "# training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# # training_sequences = np.array(training_sequences)\n",
    "# training_padded = np.array(training_padded)\n",
    "\n",
    "# testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "# testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# # testing_sequences = np.array(testing_sequences)\n",
    "# training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "# print(word_index)\n",
    "# print(training_padded[0])\n",
    "# print(training_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50LEptZFKXl6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Only considering the top 10000 most common words\n",
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "# We have 8675 rows\n",
    "training_size = 8675//2\n",
    "training_posts = posts[0:training_size]\n",
    "testing_posts = posts[training_size:]\n",
    "# Right now is only predicting introversion or extroversion\n",
    "training_labels = type_labels[0:training_size]\n",
    "testing_labels = type_labels[training_size:]\n",
    "\n",
    "# We only want to fit the tokenizer on the training, not the testing\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\n",
    "training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# training_sequences = np.array(training_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# testing_sequences = np.array(testing_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "print(word_index)\n",
    "print(training_padded[0])\n",
    "print(training_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a04qLxjNN8fv"
   },
   "outputs": [],
   "source": [
    "# # Commenting out for similar reasons\n",
    "\n",
    "# #Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# embedding_dim = 1\n",
    "\n",
    "# model = tf.keras.Sequential([ \n",
    "#                              tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "#                              tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#                              tf.keras.layers.Dense(24, activation='relu'),\n",
    "#                              tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfPpLGS7MunO"
   },
   "outputs": [],
   "source": [
    "#Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# ^^ actually i dont know if that is true\n",
    "embedding_dim = 256\n",
    "\n",
    "model = tf.keras.Sequential([ \n",
    "                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.LSTM(128),\n",
    "                            #  tf.keras.layers.Dense(1000, activation='relu'),\n",
    "                            #  tf.keras.layers.Dense(400, activation='relu'),\n",
    "                             tf.keras.layers.Dense(128, activation='relu'),\n",
    "                             tf.keras.layers.Dense(48, activation='relu'),\n",
    "                             tf.keras.layers.Dense(16, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6wi-9cvRtM6"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ReGWYoyXKTQ"
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "history = model.fit(training_padded, training_labels, epochs = num_epochs, validation_data=(testing_padded, testing_labels), verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5aec3IlTr7I"
   },
   "source": [
    "^^ This is super overfit. Probably should have stopped at epoch 10 because the validation accuracy starts decreasing. Also this isn't even using an LSTM or batching, so those need to be implemented."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mbti_classifier_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
