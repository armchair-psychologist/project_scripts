{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "mJYuGsKFeGUz",
    "outputId": "16727bbe-5a32-46fc-89e2-24cd7570a3ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode \n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "POSTS = \"posts\"\n",
    "TYPE = \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3ksc7-r5nDNh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This will load the csv\n",
    "'''\n",
    "\n",
    "class CsvToDf:\n",
    "    '''\n",
    "    This class will simply turn the given data to a dataframe\n",
    "    '''\n",
    "    def __init__(self,filename,batchSize=None,cols=None,preProc=False,postCol=False,toReplace=None):\n",
    "        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n",
    "        #into memory\n",
    "        self._toReplace = toReplace\n",
    "        self._preProc = preProc\n",
    "        self._postCol = postCol\n",
    "        self._cols = cols\n",
    "        self._header = None\n",
    "        self._filename = filename\n",
    "        self._curIndex = 0     #this will be the current index that we are in the csv\n",
    "        self._isRead = False\n",
    "        self._df = None\n",
    "        self._storeHeader()\n",
    "        self._batchSize = batchSize\n",
    "    def preprocessing_v1(self,text):\n",
    "        #remove html information\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        processed = soup.get_text(separator=\" \")\n",
    "\n",
    "        #remove http// \n",
    "        processed = re.sub(r\"http\\S+\", \"\", processed)\n",
    "\n",
    "        #remove ||| seperate\n",
    "        processed = re.sub(r'\\|\\|\\|', r' ', processed)\n",
    "\n",
    "        #lower case\n",
    "        processed = processed.lower()\n",
    "\n",
    "        #expand shortened words, e.g. don't to do not\n",
    "        processed = contractions.fix(processed)\n",
    "\n",
    "        #remove accented char\n",
    "        processed = unidecode.unidecode(processed)\n",
    "\n",
    "        #remove white space\n",
    "        #processed = processed.strip()\n",
    "        #processed = \" \".join(processed.split())\n",
    "\n",
    "        # Lemmatizing \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        processed=lemmatizer.lemmatize(processed)\n",
    "        return processed\n",
    "    def _storeHeader(self):\n",
    "        with open(self._filename) as csvFile:\n",
    "            f = csv.reader(csvFile)\n",
    "            self._header = next(f)\n",
    "    def getWholeCsv(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename)\n",
    "            self._isRead = True\n",
    "        return self._df\n",
    "    def getHeader(self):\n",
    "        return self._header\n",
    "    def _checkIfRead(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n",
    "            self._isRead = True\n",
    "            return False\n",
    "        return True\n",
    "    def removeWord(self,text):\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        words = text.split()\n",
    "        cleanWords = [i for i in words if i not in self._toReplace]\n",
    "        return \" \".join(cleanWords)\n",
    "    def getNextBatchCsv(self):\n",
    "        self._checkIfRead()\n",
    "        out = next(self._df,None)\n",
    "        if self._preProc and isinstance(out,pd.DataFrame):\n",
    "            out[self._postCol] = out[self._postCol].apply(self.preprocessing_v1)\n",
    "        if self._toReplace != None and isinstance(out,pd.DataFrame):\n",
    "            out[self._postCol] = out[self._postCol].apply(self.removeWord)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ counting the smallest number of data\n",
    "TEST = \"test\"\n",
    "TRAIN = \"train\"\n",
    "class Combiner:\n",
    "    '''\n",
    "    - Given multiple CsvToDf that correspond to a dataset combine them to a single dataframe\n",
    "    - return this dataframe\n",
    "    - need to return a dataframe that only has type and post as its columns\n",
    "    '''\n",
    "    def __init__(self,dataList,columnList):\n",
    "        '''\n",
    "        dataList is the CsvToDf that contains all the data and columnList is a list that contains the necessary\n",
    "        column names for a corresponding entry in dataList.\n",
    "        '''\n",
    "        assert len(dataList) == len(columnList),\"incorrect sizes for data\"\n",
    "        self._dataList = dataList\n",
    "        self._data = [None for i in range(len(dataList))]\n",
    "        self._necessaryCol = columnList\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._incrementData()\n",
    "    def getNextBatch(self):\n",
    "        '''\n",
    "        return a dataframe that contains all the aggregated data\n",
    "        '''\n",
    "        outData = pd.DataFrame(columns=[self._typeCol,self._postCol])\n",
    "        for data,colList in zip(self._data,self._necessaryCol):\n",
    "            if isinstance(data,pd.DataFrame):\n",
    "                renamedData = data[[colList[0],colList[1]]]\n",
    "                renamedData.columns = [self._typeCol,self._postCol]\n",
    "                \n",
    "                outData = outData.append(renamedData,ignore_index=True)\n",
    "        self._incrementData()\n",
    "        if (len(outData.index)) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return outData\n",
    "    def _incrementData(self):\n",
    "        for idx,i in enumerate(self._dataList):\n",
    "            self._data[idx] = i.getNextBatchCsv()\n",
    "class Balancer:\n",
    "    '''\n",
    "    - Balance the count\n",
    "    - Decide what the training and test dat will be\n",
    "    - Needs to output three data frames the train the test and the remainder\n",
    "    - make the remainder the training set\n",
    "    '''\n",
    "    def __init__(self,combiner,trainFreq,testFreq):\n",
    "        #personSize is minimum size of the number of people in a single personality group\n",
    "        self._combiner = combiner\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "        self._trainFreq = trainFreq\n",
    "        self._testFreq = testFreq\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "    def createDataSets(self):\n",
    "        self.reset()\n",
    "        while not(self._trainIsUniform()) or not(self._testIsUniform()):\n",
    "            #the three conditionals above will check if test and train dataset have uniform data \n",
    "            batch = self._combiner.getNextBatch()\n",
    "            if not(isinstance(batch,pd.DataFrame)):\n",
    "                break\n",
    "            for idx,row in batch.iterrows():\n",
    "                if isinstance(row[self._typeCol],str):\n",
    "                    personality = row[self._typeCol].upper()\n",
    "                    if personality in self._personality_count:\n",
    "                        if self._personality_count[personality][TRAIN] < self._trainFreq:\n",
    "                            self._training.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TRAIN] += 1\n",
    "                        elif self._personality_count[personality][TEST] < self._testFreq:\n",
    "                            self._testing.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TEST] += 1\n",
    "        return True\n",
    "    def reset(self):\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "    def getTrainSet(self):\n",
    "        return pd.DataFrame(self._training)\n",
    "    def getTestSet(self):\n",
    "        return pd.DataFrame(self._testing)\n",
    "    def _trainIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TRAIN] < self._trainFreq:\n",
    "                return False\n",
    "        return True\n",
    "    def _testIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TEST] < self._testFreq:\n",
    "                return False\n",
    "        return True\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4ab9cb969cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcurCtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNextBatchCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpersonality_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file1' is not defined"
     ]
    }
   ],
   "source": [
    "def counter(ctd):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    curCtd = ctd.getNextBatchCsv()\n",
    "    while isinstance(curCtd,pd.DataFrame):\n",
    "        for idx,row in curCtd.iterrows():\n",
    "            if isinstance(row[\"type\"],str):\n",
    "                personality = row[\"type\"].upper()\n",
    "                if personality in personality_dict:\n",
    "                    personality_dict[personality] += 1\n",
    "        curCtd = ctd.getNextBatchCsv()\n",
    "    return personality_dict\n",
    "print(counter(file1))\n",
    "print(counter(file2))\n",
    "print(counter(file3))\n",
    "print(counter(file4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPersonalityDict():\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    for idx,keys in enumerate(personality_dict):\n",
    "        oneVec = np.zeros((16,))\n",
    "        oneVec[idx] = 1\n",
    "        personality_dict[keys] = oneVec\n",
    "    return personality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterDf(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "\n",
    "    for idx,row in df.iterrows():\n",
    "        if isinstance(row[\"type\"],str):\n",
    "            personality = row[\"type\"].upper()\n",
    "            if personality in personality_dict:\n",
    "                personality_dict[personality] += 1\n",
    "    return personality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmeDcYQsH8Jd",
    "outputId": "4b3a8878-f7b5-4b10-c2bc-b3070a11dc3d"
   },
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def convertLabels(labelDf):\n",
    "    '''\n",
    "    this will turn the string labels to floats\n",
    "    '''\n",
    "    personality_dict = getPersonalityDict()\n",
    "    type_labels = []\n",
    "    # Go through the array and turn the personality type into its corresponding number\n",
    "    for idx,personality in enumerate(labelDf):\n",
    "        if isinstance(personality,str):\n",
    "            type_labels.append(personality_dict[personality.upper()])\n",
    "    return np.array(type_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def get4Dim(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    out = [[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))]]\n",
    "    for idx,row in enumerate(df):\n",
    "        personality = row\n",
    "        if isinstance(personality,str) and personality in personality_dict:\n",
    "            personality = personality.upper()\n",
    "            if personality[0] == \"E\":\n",
    "                out[0][idx] = 1\n",
    "            if personality[1] == \"S\":\n",
    "                out[1][idx] = 1\n",
    "            if personality[2] == \"T\":\n",
    "                out[2][idx] = 1\n",
    "            if personality[3] == \"J\":\n",
    "                out[3][idx] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDimModel():\n",
    "    vocab_size = 10000\n",
    "    max_length = 2016\n",
    "    embedding_dim = 256\n",
    "    return tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(64, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(64),\n",
    "                            tf.keras.layers.Dense(1, activation='sigmoid'),])\n",
    "def train4Dim(trainPost,trainLabels,num_epochs):\n",
    "    models = [createDimModel(),createDimModel(),createDimModel(),createDimModel()]\n",
    "    for idx,dims in enumerate(trainLabels):\n",
    "        print(f\"training dim {idx+1}\")\n",
    "        models[idx].compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics = [\"accuracy\"])\n",
    "        models[idx].fit(trainPost, np.array(trainLabels[idx]), epochs = num_epochs, verbose = 1)\n",
    "    return models\n",
    "def test4Dim(models,testPost,testLabels):\n",
    "    out = np.array([])\n",
    "    for idx,model in enumerate(models):\n",
    "        if len(out) == 0:\n",
    "            out = np.array([np.squeeze(model.predict(testPost))])\n",
    "        else:\n",
    "            print(out)\n",
    "            out = np.append(out,np.squeeze(model.predict(testPost)),axis=1)\n",
    "    label = np.array([[]])\n",
    "    for i in testPost:\n",
    "        if len(out) == 0:\n",
    "            label = np.array([label])\n",
    "        else:\n",
    "            label = np.append(label,i,axis=1)\n",
    "    print(f\"total accuracy for personality classification = {np.mean((np.sum(label-out,axis=1)) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convertLabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2ef31455846c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTrainSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"posts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTrainSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtesting_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTestSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"posts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtesting_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTestSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convertLabels' is not defined"
     ]
    }
   ],
   "source": [
    "training_posts = balancer.getTrainSet()[\"posts\"]\n",
    "training_labels = convertLabels(balancer.getTrainSet()[\"type\"])\n",
    "testing_posts = balancer.getTestSet()[\"posts\"]\n",
    "testing_labels = convertLabels(balancer.getTestSet()[\"type\"])\n",
    "print(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWord(se,text):\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        words = text.split()\n",
    "        cleanWords = [i for i in words if i not in self._toReplace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfFactory(fileList,columns,replaceWords,personTrain,personTest):\n",
    "    '''\n",
    "    This will create a dataframe without the specified words\n",
    "    '''\n",
    "    files = []\n",
    "    for idx,i in enumerate(fileList):\n",
    "        files.append(CsvToDf(i,batchSize=400,preProc=True,postCol=columns[idx][1],toReplace=replaceWords))\n",
    "    combine = Combiner(files,columns)\n",
    "    balancer = Balancer(combine,personTrain,personTest)\n",
    "    balancer.createDataSets()\n",
    "    return balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "def tokenize(postsSet):\n",
    "    tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "    tokenizer.fit_on_texts(postsSet)\n",
    "    training_sequences = tokenizer.texts_to_sequences(postsSet)\n",
    "    training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = 2016)\n",
    "    # training_sequences = np.array(training_sequences)\n",
    "    training_padded = np.array(training_padded)\n",
    "    return training_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrain(fileList,columns,replaceWords,personTrain,personTest):\n",
    "    out = []\n",
    "    for i in replaceWords:\n",
    "        balancer = dfFactory(fileList,columns,i,personTrain,personTest)\n",
    "        out.append(balancer)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createModels(trainSets,toReplace):\n",
    "    '''\n",
    "    trainSets must be a list of Balancers\n",
    "    '''\n",
    "    out = []\n",
    "    for idx,balancer in enumerate(trainSets):\n",
    "        i = balancer.getTrainSet()\n",
    "        print(f\"words removed: {toReplace[idx]}\")\n",
    "        out.append((train4Dim(tokenize(i[POSTS]),get4Dim(i[TYPE]),1),balancer.getTestSet()))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels(models):\n",
    "    '''\n",
    "    models will be a list of list containing a model for each dimension and each model with some words taken out\n",
    "    '''\n",
    "    for i in models:\n",
    "        test4Dim(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesList = [\"../data/mbti_1.csv\"]\n",
    "columns = [[\"type\",\"posts\"]]\n",
    "toReplace = [[],[\"suspect\"],[\"teaching\"]]\n",
    "trainSets = createTrain(filesList,columns,toReplace,30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words removed: []\n",
      "training dim 1\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7074 - accuracy: 0.5047\n",
      "training dim 2\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7045 - accuracy: 0.5341\n",
      "training dim 3\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7007 - accuracy: 0.4841\n",
      "training dim 4\n",
      "15/15 [==============================] - 22s 1s/step - loss: 0.6978 - accuracy: 0.5056\n",
      "words removed: ['suspect']\n",
      "training dim 1\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7010 - accuracy: 0.4881\n",
      "training dim 2\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.7129 - accuracy: 0.4432\n",
      "training dim 3\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7025 - accuracy: 0.4913\n",
      "training dim 4\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6954 - accuracy: 0.5253\n",
      "words removed: ['teaching']\n",
      "training dim 1\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7287 - accuracy: 0.5177\n",
      "training dim 2\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7104 - accuracy: 0.4645\n",
      "training dim 3\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6979 - accuracy: 0.4820\n",
      "training dim 4\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6987 - accuracy: 0.5222\n"
     ]
    }
   ],
   "source": [
    "models = createModels(trainSets,toReplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0][0].predict(tokenize(trainSets[0][\"posts\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(get4Dim(trainSets[0][\"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_full_pull.csv\",batchSize=400)\n",
    "file2 = CsvToDf(\"../data/mbti9k_comments.csv\",batchSize=100) \n",
    "file3 = CsvToDf(\"../data/typed_posts.csv\",batchSize=100)\n",
    "file4 = CsvToDf(\"../data/typed_comments.csv\",batchSize=100)\n",
    "\n",
    "combine = Combiner([file1,file2,file3,file4],[[\"subreddit\",\"body\"],[\"type\",\"comment\"],[\"type\",\"title\"],[\"type\",\"comment\"]])\n",
    "balancer = Balancer(combine,80,30)\n",
    "balancer.createDataSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_1.csv\",preProc=True,postCol=\"posts\")\n",
    "training_size = 6675\n",
    "df = file1.getWholeCsv()\n",
    "training_labels = get4Dim(df[0:training_size][\"type\"])\n",
    "testing_labels = get4Dim(df[training_size:][\"type\"])\n",
    "testing_posts = df[training_size:][\"posts\"]\n",
    "training_posts = df[0:training_size][\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainSet(training_posts,wordList):\n",
    "    out = []\n",
    "    for i in wordList:\n",
    "        out.append(training_posts.apply(replaceWord,keyWords=i))\n",
    "    return out\n",
    "def replaceWord(text,keyWords):\n",
    "    print(text)\n",
    "    if not(isinstance(text,str)):\n",
    "        return text\n",
    "    words = text.split()\n",
    "    cleanWords = [i for i in words if i not in keyWords]\n",
    "    return \" \".join(cleanWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = createTrainSet(training_posts,[[\"Dear\",\"Good\",\"i\"],[\"test\"]])\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = get4Dim(balancer.getTrainSet()[\"type\"])\n",
    "testing_labels = get4Dim(balancer.getTestSet()[\"type\"])\n",
    "testing_posts = balancer.getTestSet()[\"posts\"]\n",
    "training_posts = balancer.getTrainSet()[\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50LEptZFKXl6",
    "outputId": "566e4359-729c-4d9b-9f93-8d0e8e1f7858"
   },
   "outputs": [],
   "source": [
    "# Only considering the top 10000 most common words\n",
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "# We only want to fit the tokenizer on the training, not the testing\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\n",
    "training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# training_sequences = np.array(training_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# testing_sequences = np.array(testing_sequences)\n",
    "testing_padded = np.array(testing_padded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dim 1\n",
      "Epoch 1/5\n",
      "209/209 [==============================] - 434s 2s/step - loss: 0.5537 - accuracy: 0.7514\n",
      "Epoch 2/5\n",
      "209/209 [==============================] - 435s 2s/step - loss: 0.5443 - accuracy: 0.7681\n",
      "Epoch 3/5\n",
      " 55/209 [======>.......................] - ETA: 5:21 - loss: 0.5183 - accuracy: 0.7876"
     ]
    }
   ],
   "source": [
    "models = train4Dim(training_padded,training_labels,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for dim 1 personality classification = 0.4395833333333333\n",
      "accuracy for dim 2 personality classification = 0.5166666666666667\n",
      "accuracy for dim 3 personality classification = 0.47291666666666665\n",
      "accuracy for dim 4 personality classification = 0.5145833333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getAccuracy(models,testing_labels):\n",
    "    for idx,model in enumerate(models):\n",
    "        test = testing_labels[idx]\n",
    "        modelOut = np.round(models[idx].predict(testing_padded))\n",
    "        print(f\"accuracy for dim {idx+1} personality classification = {np.mean(abs(np.squeeze(modelOut)-np.squeeze(test)) == 0)}\")\n",
    "getAccuracy(models,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy for personality classification = 0.03958333333333333\n"
     ]
    }
   ],
   "source": [
    "def getTotalAccuracy(models,testing_labels):\n",
    "    total = None\n",
    "    isEmpty = True\n",
    "    for idx,model in enumerate(models):\n",
    "        test = testing_labels[idx]\n",
    "        modelOut = np.squeeze(np.round(models[idx].predict(testing_padded)))\n",
    "        if isEmpty:\n",
    "            total = np.array(modelOut)\n",
    "            isEmpty = False\n",
    "        else:\n",
    "            total = np.column_stack((total,modelOut))\n",
    "    labels = None\n",
    "    isEmpty = True\n",
    "    for idx,col in enumerate(testing_labels):\n",
    "        if isEmpty:\n",
    "            labels = np.array(col)\n",
    "            isEmpty = False\n",
    "        else:\n",
    "            labels = np.column_stack((labels,col))\n",
    "    print(f\"total accuracy for personality classification = {np.mean(np.sum(abs(total-labels),axis=1) == 0)}\")\n",
    "getTotalAccuracy(models,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4Dim(models,testing_padded,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AfPpLGS7MunO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# ^^ actually i dont know if that is true\n",
    "embedding_dim = 256\n",
    "'''\n",
    "Embedding layer will always have vocab_size*embedding_dim parameters. Since vocab_size is 10,000 the number of parameters on this layer will always be large\n",
    "'''\n",
    "model = tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(256, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(128),\n",
    "                            tf.keras.layers.Dense(16, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = 'sgd', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6wi-9cvRtM6",
    "outputId": "30be599c-d625-4336-af86-7462bd9b982f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2016, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 2016, 256)         393984    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 3,005,328\n",
      "Trainable params: 3,005,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs = num_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      " 128/1600 [=>............................] - ETA: 3:03 - loss: 2.8088 - acc: 0.0547"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpMLBWYmzYM5"
   },
   "outputs": [],
   "source": [
    "res = np.argmax(model.predict(testing_padded),axis=1)\n",
    "label = np.argmax(testing_labels,axis=1)\n",
    "print(f\"accuracy = {np.mean((label-res) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mbti_classifier_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
