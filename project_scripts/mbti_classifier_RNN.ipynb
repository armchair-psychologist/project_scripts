{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "mJYuGsKFeGUz",
    "outputId": "16727bbe-5a32-46fc-89e2-24cd7570a3ec"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3ksc7-r5nDNh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This will load the csv\n",
    "'''\n",
    "class CsvToDf:\n",
    "    '''\n",
    "    This class will simply turn the given data to a dataframe\n",
    "    '''\n",
    "    def __init__(self,filename,batchSize=None,cols=None):\n",
    "        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n",
    "        #into memory\n",
    "        self._cols = cols\n",
    "        self._header = None\n",
    "        self._filename = filename\n",
    "        self._curIndex = 0     #this will be the current index that we are in the csv\n",
    "        self._isRead = False\n",
    "        self._df = None\n",
    "        self._storeHeader()\n",
    "        self._batchSize = batchSize\n",
    "    def _storeHeader(self):\n",
    "        with open(self._filename) as csvFile:\n",
    "            f = csv.reader(csvFile)\n",
    "            self._header = next(f)\n",
    "    def getWholeCsv(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename)\n",
    "            self._isRead = True\n",
    "        return self._df\n",
    "    def getHeader(self):\n",
    "        return self._header\n",
    "    def _checkIfRead(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n",
    "            self._isRead = True\n",
    "            return False\n",
    "        return True\n",
    "    def getNextBatchCsv(self):\n",
    "        self._checkIfRead()\n",
    "        return next(self._df,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ counting the smallest number of data\n",
    "TEST = \"test\"\n",
    "TRAIN = \"train\"\n",
    "class Combiner:\n",
    "    '''\n",
    "    - Given multiple CsvToDf that correspond to a dataset combine them to a single dataframe\n",
    "    - return this dataframe\n",
    "    - need to return a dataframe that only has type and post as its columns\n",
    "    '''\n",
    "    def __init__(self,dataList,columnList):\n",
    "        '''\n",
    "        dataList is the CsvToDf that contains all the data and columnList is a list that contains the necessary\n",
    "        column names for a corresponding entry in dataList.\n",
    "        '''\n",
    "        assert len(dataList) == len(columnList),\"incorrect sizes for data\"\n",
    "        self._dataList = dataList\n",
    "        self._data = [None for i in range(len(dataList))]\n",
    "        self._necessaryCol = columnList\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._incrementData()\n",
    "    def getNextBatch(self):\n",
    "        '''\n",
    "        return a dataframe that contains all the aggregated data\n",
    "        '''\n",
    "        outData = pd.DataFrame(columns=[self._typeCol,self._postCol])\n",
    "        for data,colList in zip(self._data,self._necessaryCol):\n",
    "            if isinstance(data,pd.DataFrame):\n",
    "                renamedData = data[[colList[0],colList[1]]]\n",
    "                renamedData.columns = [self._typeCol,self._postCol]\n",
    "                \n",
    "                outData = outData.append(renamedData,ignore_index=True)\n",
    "        self._incrementData()\n",
    "        if (len(outData.index)) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return outData\n",
    "    def _incrementData(self):\n",
    "        for idx,i in enumerate(self._dataList):\n",
    "            self._data[idx] = i.getNextBatchCsv()\n",
    "class Balancer:\n",
    "    '''\n",
    "    - Balance the count\n",
    "    - Decide what the training and test dat will be\n",
    "    - Needs to output three data frames the train the test and the remainder\n",
    "    - make the remainder the training set\n",
    "    '''\n",
    "    def __init__(self,combiner,trainFreq,testFreq):\n",
    "        #personSize is minimum size of the number of people in a single personality group\n",
    "        self._combiner = combiner\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "        self._trainFreq = trainFreq\n",
    "        self._testFreq = testFreq\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "    def createDataSets(self):\n",
    "        self.reset()\n",
    "        while not(self._trainIsUniform()) or not(self._testIsUniform()):\n",
    "            #the three conditionals above will check if test and train dataset have uniform data \n",
    "            batch = self._combiner.getNextBatch()\n",
    "            if not(isinstance(batch,pd.DataFrame)):\n",
    "                break\n",
    "            for idx,row in batch.iterrows():\n",
    "                if isinstance(row[self._typeCol],str):\n",
    "                    personality = row[self._typeCol].upper()\n",
    "                    if personality in self._personality_count:\n",
    "                        if self._personality_count[personality][TRAIN] < self._trainFreq:\n",
    "                            self._training.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TRAIN] += 1\n",
    "                        elif self._personality_count[personality][TEST] < self._testFreq:\n",
    "                            self._testing.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TEST] += 1\n",
    "        return True\n",
    "    def reset(self):\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "    def getTrainSet(self):\n",
    "        return pd.DataFrame(self._training)\n",
    "    def getTestSet(self):\n",
    "        return pd.DataFrame(self._testing)\n",
    "    def _trainIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TRAIN] < self._trainFreq:\n",
    "                return False\n",
    "        return True\n",
    "    def _testIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TEST] < self._testFreq:\n",
    "                return False\n",
    "        return True\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4ab9cb969cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcurCtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNextBatchCsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpersonality_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file1' is not defined"
     ]
    }
   ],
   "source": [
    "def counter(ctd):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    curCtd = ctd.getNextBatchCsv()\n",
    "    while isinstance(curCtd,pd.DataFrame):\n",
    "        for idx,row in curCtd.iterrows():\n",
    "            if isinstance(row[\"type\"],str):\n",
    "                personality = row[\"type\"].upper()\n",
    "                if personality in personality_dict:\n",
    "                    personality_dict[personality] += 1\n",
    "        curCtd = ctd.getNextBatchCsv()\n",
    "    return personality_dict\n",
    "print(counter(file1))\n",
    "print(counter(file2))\n",
    "print(counter(file3))\n",
    "print(counter(file4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_full_pull.csv\",batchSize=400)\n",
    "file2 = CsvToDf(\"../data/mbti9k_comments.csv\",batchSize=100) \n",
    "file3 = CsvToDf(\"../data/typed_posts.csv\",batchSize=100)\n",
    "file4 = CsvToDf(\"../data/typed_comments.csv\",batchSize=100)\n",
    "\n",
    "combine = Combiner([file1,file2,file3,file4],[[\"subreddit\",\"body\"],[\"type\",\"comment\"],[\"type\",\"title\"],[\"type\",\"comment\"]])\n",
    "balancer = Balancer(combine,100,10)\n",
    "balancer.createDataSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENTJ': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INTJ': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ENTP': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INTP': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INFJ': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'INFP': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ENFJ': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ENFP': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), 'ESTP': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), 'ESTJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), 'ISTP': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'ISTJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), 'ISFJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), 'ISFP': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), 'ESFJ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'ESFP': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "def getPersonalityDict():\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    for idx,keys in enumerate(personality_dict):\n",
    "        oneVec = np.zeros((16,))\n",
    "        oneVec[idx] = 1\n",
    "        personality_dict[keys] = oneVec\n",
    "    return personality_dict\n",
    "print(getPersonalityDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterDf(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "\n",
    "    for idx,row in df.iterrows():\n",
    "        if isinstance(row[\"type\"],str):\n",
    "            personality = row[\"type\"].upper()\n",
    "            if personality in personality_dict:\n",
    "                personality_dict[personality] += 1\n",
    "    return personality_dict\n",
    "counterDf(balancer.getTrainSet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmeDcYQsH8Jd",
    "outputId": "4b3a8878-f7b5-4b10-c2bc-b3070a11dc3d"
   },
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def convertLabels(labelDf):\n",
    "    '''\n",
    "    this will turn the string labels to floats\n",
    "    '''\n",
    "    personality_dict = getPersonalityDict()\n",
    "    type_labels = []\n",
    "    # Go through the array and turn the personality type into its corresponding number\n",
    "    for idx,personality in enumerate(labelDf):\n",
    "        if isinstance(personality,str):\n",
    "            type_labels.append(personality_dict[personality.upper()])\n",
    "    return np.array(type_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def get4Dim(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    out = [[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))]]\n",
    "    for idx,row in enumerate(df):\n",
    "        personality = row\n",
    "        if isinstance(personality,str) and personality in personality_dict:\n",
    "            personality = personality.upper()\n",
    "            if personality[0] == \"E\":\n",
    "                out[0][idx] = 1\n",
    "            if personality[1] == \"S\":\n",
    "                out[1][idx] = 1\n",
    "            if personality[2] == \"T\":\n",
    "                out[2][idx] = 1\n",
    "            if personality[3] == \"J\":\n",
    "                out[3][idx] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "training_posts = balancer.getTrainSet()[\"posts\"]\n",
    "training_labels = get4Dim(balancer.getTrainSet()[\"type\"])\n",
    "testing_posts = balancer.getTestSet()[\"posts\"]\n",
    "testing_labels = get4Dim(balancer.getTestSet()[\"type\"])\n",
    "print(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDimModel():\n",
    "    vocab_size = 10000\n",
    "    max_length = 2016\n",
    "    embedding_dim = 256\n",
    "    return tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(256, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(128),\n",
    "                            tf.keras.layers.Dense(1, activation='softmax'),])\n",
    "def train4Dim(trainPost,trainLabels,num_epochs):\n",
    "    models = [createDimModel(),createDimModel(),createDimModel(),createDimModel()]\n",
    "    for idx,dims in enumerate(trainLabels):\n",
    "        models[idx].compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = 'sgd', metrics = [\"accuracy\"])\n",
    "        models[idx].fit(trainPost, np.array(trainLabels[idx]), epochs = num_epochs, verbose = 1)\n",
    "    return models\n",
    "def test4Dim(models,testPost,testLabels):\n",
    "    for idx,model in models:\n",
    "        res = np.round(model.predict(testPost))\n",
    "        label = testLabels\n",
    "        print(f\"model {idx} accuracy = {np.mean((label-res) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "training_posts = balancer.getTrainSet()[\"posts\"]\n",
    "training_labels = convertLabels(balancer.getTrainSet()[\"type\"])\n",
    "testing_posts = balancer.getTestSet()[\"posts\"]\n",
    "testing_labels = convertLabels(balancer.getTestSet()[\"type\"])\n",
    "print(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50LEptZFKXl6",
    "outputId": "566e4359-729c-4d9b-9f93-8d0e8e1f7858"
   },
   "outputs": [],
   "source": [
    "# Only considering the top 10000 most common words\n",
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "# We only want to fit the tokenizer on the training, not the testing\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\n",
    "training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# training_sequences = np.array(training_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# testing_sequences = np.array(testing_sequences)\n",
    "testing_padded = np.array(testing_padded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 419s 8s/step - loss: 0.6987 - accuracy: 0.4894\n",
      "50/50 [==============================] - 403s 8s/step - loss: 0.6774 - accuracy: 0.4891\n",
      "50/50 [==============================] - 380s 8s/step - loss: 0.6975 - accuracy: 0.4930\n",
      "50/50 [==============================] - 375s 7s/step - loss: 0.6968 - accuracy: 0.4972\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-348c81fe7c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain4Dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest4Dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtesting_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtesting_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-4d93149ddab5>\u001b[0m in \u001b[0;36mtest4Dim\u001b[0;34m(models, testPost, testLabels)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest4Dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestPost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestPost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not iterable"
     ]
    }
   ],
   "source": [
    "models = train4Dim(training_padded,training_labels,1)\n",
    "test4Dim(models,testing_padded,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AfPpLGS7MunO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# ^^ actually i dont know if that is true\n",
    "embedding_dim = 256\n",
    "'''\n",
    "Embedding layer will always have vocab_size*embedding_dim parameters. Since vocab_size is 10,000 the number of parameters on this layer will always be large\n",
    "'''\n",
    "model = tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(256, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(128),\n",
    "                            tf.keras.layers.Dense(16, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = 'sgd', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6wi-9cvRtM6",
    "outputId": "30be599c-d625-4336-af86-7462bd9b982f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2016, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 2016, 256)         393984    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 3,005,328\n",
      "Trainable params: 3,005,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs = num_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      " 128/1600 [=>............................] - ETA: 3:03 - loss: 2.8088 - acc: 0.0547"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpMLBWYmzYM5"
   },
   "outputs": [],
   "source": [
    "res = np.argmax(model.predict(testing_padded),axis=1)\n",
    "label = np.argmax(testing_labels,axis=1)\n",
    "print(f\"accuracy = {np.mean((label-res) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mbti_classifier_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
