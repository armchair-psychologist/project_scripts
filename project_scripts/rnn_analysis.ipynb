{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "mJYuGsKFeGUz",
    "outputId": "16727bbe-5a32-46fc-89e2-24cd7570a3ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode \n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "POSTS = \"posts\"\n",
    "TYPE = \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3ksc7-r5nDNh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This will load the csv\n",
    "'''\n",
    "\n",
    "class CsvToDf:\n",
    "    '''\n",
    "    This class will simply turn the given data to a dataframe\n",
    "    '''\n",
    "    def __init__(self,filename,batchSize=None,cols=None,preProc=False,postCol=False,toReplace=None):\n",
    "        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n",
    "        #into memory\n",
    "        self._toReplace = toReplace\n",
    "        self._preProc = preProc\n",
    "        self._postCol = postCol\n",
    "        self._cols = cols\n",
    "        self._header = None\n",
    "        self._filename = filename\n",
    "        self._curIndex = 0     #this will be the current index that we are in the csv\n",
    "        self._isRead = False\n",
    "        self._df = None\n",
    "        self._storeHeader()\n",
    "        self._batchSize = batchSize\n",
    "    def preprocessing_v1(self,text):\n",
    "        #remove html information\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        processed = soup.get_text(separator=\" \")\n",
    "\n",
    "        #remove http// \n",
    "        processed = re.sub(r\"http\\S+\", \"\", processed)\n",
    "\n",
    "        #remove ||| seperate\n",
    "        processed = re.sub(r'\\|\\|\\|', r' ', processed)\n",
    "\n",
    "        #lower case\n",
    "        processed = processed.lower()\n",
    "\n",
    "        #expand shortened words, e.g. don't to do not\n",
    "        processed = contractions.fix(processed)\n",
    "\n",
    "        #remove accented char\n",
    "        processed = unidecode.unidecode(processed)\n",
    "\n",
    "        #remove white space\n",
    "        #processed = processed.strip()\n",
    "        #processed = \" \".join(processed.split())\n",
    "\n",
    "        # Lemmatizing \n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        processed=lemmatizer.lemmatize(processed)\n",
    "        return processed\n",
    "    def _storeHeader(self):\n",
    "        with open(self._filename) as csvFile:\n",
    "            f = csv.reader(csvFile)\n",
    "            self._header = next(f)\n",
    "    def getWholeCsv(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename)\n",
    "            self._isRead = True\n",
    "        if self._preProc:\n",
    "            self._df[self._postCol] = self._df[self._postCol].apply(self.preprocessing_v1)\n",
    "        return self._df\n",
    "    def getHeader(self):\n",
    "        return self._header\n",
    "    def _checkIfRead(self):\n",
    "        if not(self._isRead):\n",
    "            if self._cols != None:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n",
    "            else:\n",
    "                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n",
    "            self._isRead = True\n",
    "            return False\n",
    "        return True\n",
    "    def removeWord(self,text):\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        words = text.split()\n",
    "        cleanWords = [i for i in words if i not in self._toReplace]\n",
    "        return \" \".join(cleanWords)\n",
    "    def getNextBatchCsv(self):\n",
    "        self._checkIfRead()\n",
    "        out = next(self._df,None)\n",
    "        if self._preProc and isinstance(out,pd.DataFrame):\n",
    "            out[self._postCol] = out[self._postCol].apply(self.preprocessing_v1)\n",
    "        if self._toReplace != None and isinstance(out,pd.DataFrame):\n",
    "            out[self._postCol] = out[self._postCol].apply(self.removeWord)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ counting the smallest number of data\n",
    "TEST = \"test\"\n",
    "TRAIN = \"train\"\n",
    "class Combiner:\n",
    "    '''\n",
    "    - Given multiple CsvToDf that correspond to a dataset combine them to a single dataframe\n",
    "    - return this dataframe\n",
    "    - need to return a dataframe that only has type and post as its columns\n",
    "    '''\n",
    "    def __init__(self,dataList,columnList):\n",
    "        '''\n",
    "        dataList is the CsvToDf that contains all the data and columnList is a list that contains the necessary\n",
    "        column names for a corresponding entry in dataList.\n",
    "        '''\n",
    "        assert len(dataList) == len(columnList),\"incorrect sizes for data\"\n",
    "        self._dataList = dataList\n",
    "        self._data = [None for i in range(len(dataList))]\n",
    "        self._necessaryCol = columnList\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._incrementData()\n",
    "    def getNextBatch(self):\n",
    "        '''\n",
    "        return a dataframe that contains all the aggregated data\n",
    "        '''\n",
    "        outData = pd.DataFrame(columns=[self._typeCol,self._postCol])\n",
    "        for data,colList in zip(self._data,self._necessaryCol):\n",
    "            if isinstance(data,pd.DataFrame):\n",
    "                renamedData = data[[colList[0],colList[1]]]\n",
    "                renamedData.columns = [self._typeCol,self._postCol]\n",
    "                \n",
    "                outData = outData.append(renamedData,ignore_index=True)\n",
    "        self._incrementData()\n",
    "        if (len(outData.index)) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return outData\n",
    "    def _incrementData(self):\n",
    "        for idx,i in enumerate(self._dataList):\n",
    "            self._data[idx] = i.getNextBatchCsv()\n",
    "class Balancer:\n",
    "    '''\n",
    "    - Balance the count\n",
    "    - Decide what the training and test dat will be\n",
    "    - Needs to output three data frames the train the test and the remainder\n",
    "    - make the remainder the training set\n",
    "    '''\n",
    "    def __init__(self,combiner,trainFreq,testFreq):\n",
    "        #personSize is minimum size of the number of people in a single personality group\n",
    "        self._combiner = combiner\n",
    "        self._typeCol = \"type\"\n",
    "        self._postCol = \"posts\"\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "        self._trainFreq = trainFreq\n",
    "        self._testFreq = testFreq\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "    def createDataSets(self):\n",
    "        self.reset()\n",
    "        while not(self._trainIsUniform()) or not(self._testIsUniform()):\n",
    "            #the three conditionals above will check if test and train dataset have uniform data \n",
    "            batch = self._combiner.getNextBatch()\n",
    "            if not(isinstance(batch,pd.DataFrame)):\n",
    "                break\n",
    "            for idx,row in batch.iterrows():\n",
    "                if isinstance(row[self._typeCol],str):\n",
    "                    personality = row[self._typeCol].upper()\n",
    "                    if personality in self._personality_count:\n",
    "                        if self._personality_count[personality][TRAIN] < self._trainFreq:\n",
    "                            self._training.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TRAIN] += 1\n",
    "                        elif self._personality_count[personality][TEST] < self._testFreq:\n",
    "                            self._testing.append({self._typeCol:personality,self._postCol:row[self._postCol]})\n",
    "                            self._personality_count[personality][TEST] += 1\n",
    "        return True\n",
    "    def reset(self):\n",
    "        self._training = []\n",
    "        self._testing = []\n",
    "        self._personality_count = {\"ENTJ\" : {TRAIN:0,TEST:0}, \"INTJ\" : {TRAIN:0,TEST:0}, \"ENTP\" : {TRAIN:0,TEST:0}, \"INTP\" : {TRAIN:0,TEST:0}, \"INFJ\" : {TRAIN:0,TEST:0}, \"INFP\" : {TRAIN:0,TEST:0}, \"ENFJ\" : {TRAIN:0,TEST:0} , \n",
    "                    \"ENFP\" : {TRAIN:0,TEST:0}, \"ESTP\" : {TRAIN:0,TEST:0}, \"ESTJ\" : {TRAIN:0,TEST:0}, \"ISTP\" : {TRAIN:0,TEST:0}, \"ISTJ\" : {TRAIN:0,TEST:0}, \"ISFJ\" : {TRAIN:0,TEST:0}, \"ISFP\" : {TRAIN:0,TEST:0}, \n",
    "                    \"ESFJ\" : {TRAIN:0,TEST:0}, \"ESFP\" : {TRAIN:0,TEST:0}}\n",
    "    def getTrainSet(self):\n",
    "        return pd.DataFrame(self._training)\n",
    "    def getTestSet(self):\n",
    "        return pd.DataFrame(self._testing)\n",
    "    def _trainIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TRAIN] < self._trainFreq:\n",
    "                return False\n",
    "        return True\n",
    "    def _testIsUniform(self):\n",
    "        #checks if personality count has equal distribution\n",
    "        for key in self._personality_count:\n",
    "            if self._personality_count[key][TEST] < self._testFreq:\n",
    "                return False\n",
    "        return True\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPersonalityDict():\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    for idx,keys in enumerate(personality_dict):\n",
    "        oneVec = np.zeros((16,))\n",
    "        oneVec[idx] = 1\n",
    "        personality_dict[keys] = oneVec\n",
    "    return personality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterDf(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "\n",
    "    for idx,row in df.iterrows():\n",
    "        if isinstance(row[\"type\"],str):\n",
    "            personality = row[\"type\"].upper()\n",
    "            if personality in personality_dict:\n",
    "                personality_dict[personality] += 1\n",
    "    return personality_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmeDcYQsH8Jd",
    "outputId": "4b3a8878-f7b5-4b10-c2bc-b3070a11dc3d"
   },
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def convertLabels(labelDf):\n",
    "    '''\n",
    "    this will turn the string labels to floats\n",
    "    '''\n",
    "    personality_dict = getPersonalityDict()\n",
    "    type_labels = []\n",
    "    # Go through the array and turn the personality type into its corresponding number\n",
    "    for idx,personality in enumerate(labelDf):\n",
    "        if isinstance(personality,str):\n",
    "            type_labels.append(personality_dict[personality.upper()])\n",
    "    return np.array(type_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"type\"\n",
    "def get4Dim(df):\n",
    "    personality_dict = {\"ENTJ\" : 0, \"INTJ\" : 0, \"ENTP\" : 0, \"INTP\" : 0, \"INFJ\" : 0, \"INFP\" : 0, \"ENFJ\" : 0, \n",
    "                    \"ENFP\" : 0, \"ESTP\" : 0, \"ESTJ\" : 0, \"ISTP\" : 0, \"ISTJ\" : 0, \"ISFJ\" : 0, \"ISFP\" : 0, \n",
    "                    \"ESFJ\" : 0, \"ESFP\" : 0}\n",
    "    out = [[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))],[0 for i in range(len(df.index))]]\n",
    "    for idx,row in enumerate(df):\n",
    "        personality = row\n",
    "        if isinstance(personality,str) and personality in personality_dict:\n",
    "            personality = personality.upper()\n",
    "            if personality[0] == \"E\":\n",
    "                out[0][idx] = 1\n",
    "            if personality[1] == \"S\":\n",
    "                out[1][idx] = 1\n",
    "            if personality[2] == \"T\":\n",
    "                out[2][idx] = 1\n",
    "            if personality[3] == \"J\":\n",
    "                out[3][idx] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDimModel():\n",
    "    vocab_size = 10000\n",
    "    max_length = 2016\n",
    "    embedding_dim = 10\n",
    "    return tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(64, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(64),\n",
    "                            tf.keras.layers.Dense(1, activation='sigmoid'),])\n",
    "def train4Dim(trainPost,trainLabels,num_epochs):\n",
    "    models = [createDimModel(),createDimModel(),createDimModel(),createDimModel()]\n",
    "    for idx,dims in enumerate(trainLabels):\n",
    "        print(f\"training dim {idx+1}\")\n",
    "        models[idx].compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics = [\"accuracy\"])\n",
    "        models[idx].fit(trainPost, np.array(trainLabels[idx]), epochs = num_epochs, verbose = 1)\n",
    "    return models\n",
    "def test4Dim(models,testPost,testLabels):\n",
    "    out = np.array([])\n",
    "    for idx,model in enumerate(models):\n",
    "        if len(out) == 0:\n",
    "            out = np.array([np.squeeze(model.predict(testPost))])\n",
    "        else:\n",
    "            print(out)\n",
    "            out = np.append(out,np.squeeze(model.predict(testPost)),axis=1)\n",
    "    label = np.array([[]])\n",
    "    for i in testPost:\n",
    "        if len(out) == 0:\n",
    "            label = np.array([label])\n",
    "        else:\n",
    "            label = np.append(label,i,axis=1)\n",
    "    print(f\"total accuracy for personality classification = {np.mean((np.sum(label-out,axis=1)) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWord(se,text):\n",
    "        if not(isinstance(text,str)):\n",
    "            return text\n",
    "        words = text.split()\n",
    "        cleanWords = [i for i in words if i not in self._toReplace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfFactory(fileList,columns,replaceWords,personTrain,personTest):\n",
    "    '''\n",
    "    This will create a dataframe without the specified words\n",
    "    '''\n",
    "    files = []\n",
    "    for idx,i in enumerate(fileList):\n",
    "        files.append(CsvToDf(i,batchSize=400,preProc=True,postCol=columns[idx][1],toReplace=replaceWords))\n",
    "    combine = Combiner(files,columns)\n",
    "    balancer = Balancer(combine,personTrain,personTest)\n",
    "    balancer.createDataSets()\n",
    "    return balancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "def tokenize(postsSet):\n",
    "    tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "    tokenizer.fit_on_texts(postsSet)\n",
    "    training_sequences = tokenizer.texts_to_sequences(postsSet)\n",
    "    training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = 2016)\n",
    "    # training_sequences = np.array(training_sequences)\n",
    "    training_padded = np.array(training_padded)\n",
    "    return training_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrain(fileList,columns,replaceWords,personTrain,personTest):\n",
    "    out = []\n",
    "    for i in replaceWords:\n",
    "        balancer = dfFactory(fileList,columns,i,personTrain,personTest)\n",
    "        out.append(balancer)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createModels(trainSets,toReplace):\n",
    "    '''\n",
    "    trainSets must be a list of Balancers\n",
    "    '''\n",
    "    out = []\n",
    "    for idx,balancer in enumerate(trainSets):\n",
    "        i = balancer.getTrainSet()\n",
    "        print(f\"words removed: {toReplace[idx]}\")\n",
    "        out.append((train4Dim(tokenize(i[POSTS]),get4Dim(i[TYPE]),1),balancer.getTestSet()))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels(models):\n",
    "    '''\n",
    "    models will be a list of list containing a model for each dimension and each model with some words taken out\n",
    "    '''\n",
    "    for i in models:\n",
    "        test4Dim(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_full_pull.csv\",batchSize=400)\n",
    "file2 = CsvToDf(\"../data/mbti9k_comments.csv\",batchSize=100) \n",
    "file3 = CsvToDf(\"../data/typed_posts.csv\",batchSize=100)\n",
    "file4 = CsvToDf(\"../data/typed_comments.csv\",batchSize=100)\n",
    "\n",
    "combine = Combiner([file1,file2,file3,file4],[[\"subreddit\",\"body\"],[\"type\",\"comment\"],[\"type\",\"title\"],[\"type\",\"comment\"]])\n",
    "balancer = Balancer(combine,80,30)\n",
    "balancer.createDataSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_1.csv\")\n",
    "\n",
    "combine = Combiner([file1],[[\"type\",\"posts\"]])\n",
    "balancer = Balancer(combine,39,30)\n",
    "balancer.createDataSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = get4Dim(balancer.getTrainSet()[\"type\"])\n",
    "testing_labels = get4Dim(balancer.getTestSet()[\"type\"])\n",
    "testing_posts = balancer.getTestSet()[\"posts\"]\n",
    "training_posts = balancer.getTrainSet()[\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = CsvToDf(\"../data/mbti_1.csv\",preProc=True,postCol=\"posts\")\n",
    "training_size = 6675\n",
    "df = file1.getWholeCsv()\n",
    "training_labels = get4Dim(df[0:training_size][\"type\"])\n",
    "testing_labels = get4Dim(df[training_size:][\"type\"])\n",
    "testing_posts = df[training_size:][\"posts\"]\n",
    "training_posts = df[0:training_size][\"posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = [['sweet', 'deal', 'month', 'then', 'know', 'thought', 'up.', 'off', 'asking', 'he', 'idea.', 'crush', 'this', 'college', 'it', 'most', 'dealing', 'late', 'key', 'his', 'talk', 'oh', 'those', 'everything.'],\n",
    "          ['loyalty:', 'dogmas','post.]', \"'rarity',\",'static:', 'grins.','358890', 'intimidate,','84389', '84390','beck.', 'alpha;','transversal', 'ri,'\n",
    "          ,'gosh....this', 'now.....but','error(when', 'times(asshole','oooooh...', 'floss?','rupp,', 'customization.',\n",
    "          'backstories', 'error!!!,','time...there', 'yeah...this','jimmers,', 'boat?...','fi>ti>fe>te', '-plans','hulme', 'scotland?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainSet(training_posts,wordList):\n",
    "    out = []\n",
    "    for i in wordList:\n",
    "        out.append(training_posts.apply(replaceWord,keyWords=i))\n",
    "    return out\n",
    "def replaceWord(text,keyWords):\n",
    "    if not(isinstance(text,str)):\n",
    "        return text\n",
    "    words = text.split(\" \")\n",
    "    cleanWords = [word for word in words if (word not in keyWords)]\n",
    "    return \" \".join(cleanWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = createTrainSet(training_posts,wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDf(result,labels,wordList):\n",
    "    out = []\n",
    "    for idx,i in enumerate(result):\n",
    "        print(f\"words replaced {wordList[idx]}\")\n",
    "        out.append(train4Dim(tokenize(i),labels,2))\n",
    "    return out\n",
    "def testDf(models,testSet,testLabels,replaceWords):\n",
    "    for idx,i in enumerate(models):\n",
    "        print(f\"elminated words: {replaceWords[idx]}\")\n",
    "        getAccuracy(i,testLabels,testSet)\n",
    "        getTotalAccuracy(i,testLabels,testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words replaced ['sweet', 'deal', 'month', 'then', 'know', 'thought', 'up.', 'off', 'asking', 'he', 'idea.', 'crush', 'this', 'college', 'it', 'most', 'dealing', 'late', 'key', 'his', 'talk', 'oh', 'those', 'everything.']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 45s 2s/step - loss: 0.6987 - accuracy: 0.5149\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.6928 - accuracy: 0.5173\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 42s 2s/step - loss: 0.6966 - accuracy: 0.5344\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 41s 2s/step - loss: 0.6990 - accuracy: 0.4955\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.7077 - accuracy: 0.4579\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6945 - accuracy: 0.5021\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6980 - accuracy: 0.4671\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.6943 - accuracy: 0.4970\n",
      "words replaced ['loyalty:', 'dogmas', 'post.]', \"'rarity',\", 'static:', 'grins.', '358890', 'intimidate,', '84389', '84390', 'beck.', 'alpha;', 'transversal', 'ri,', 'gosh....this', 'now.....but', 'error(when', 'times(asshole', 'oooooh...', 'floss?', 'rupp,', 'customization.', 'backstories', 'error!!!,', 'time...there', 'yeah...this', 'jimmers,', 'boat?...', 'fi>ti>fe>te', '-plans', 'hulme', 'scotland?']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 29s 1s/step - loss: 0.7021 - accuracy: 0.5267\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.7066 - accuracy: 0.5399\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6976 - accuracy: 0.5180\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.6933 - accuracy: 0.5221\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.7011 - accuracy: 0.4337\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6937 - accuracy: 0.4845\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.7064 - accuracy: 0.5241\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.7059 - accuracy: 0.5053\n"
     ]
    }
   ],
   "source": [
    "#this was trained on the balanced dataset less parameters\n",
    "models = trainDf(res,training_labels,wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words replaced ['infj']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 37s 2s/step - loss: 0.7047 - accuracy: 0.4707\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.6948 - accuracy: 0.4989\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.7057 - accuracy: 0.4808\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 35s 2s/step - loss: 0.6941 - accuracy: 0.5061\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.7156 - accuracy: 0.5096\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.7066 - accuracy: 0.5095\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 32s 1s/step - loss: 0.7167 - accuracy: 0.4871\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 29s 1s/step - loss: 0.6964 - accuracy: 0.4941\n",
      "words replaced ['intp']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.6997 - accuracy: 0.5043\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 29s 1s/step - loss: 0.7076 - accuracy: 0.4797\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.7056 - accuracy: 0.4931\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 29s 1s/step - loss: 0.6963 - accuracy: 0.4960\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.6946 - accuracy: 0.5315\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 29s 1s/step - loss: 0.7095 - accuracy: 0.5057\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 30s 1s/step - loss: 0.6942 - accuracy: 0.5330\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 29s 1s/step - loss: 0.7007 - accuracy: 0.4824\n",
      "words replaced ['infjs']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.6959 - accuracy: 0.5178\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 30s 1s/step - loss: 0.6939 - accuracy: 0.4983\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 32s 2s/step - loss: 0.6987 - accuracy: 0.5148\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 30s 1s/step - loss: 0.7055 - accuracy: 0.4373\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.7061 - accuracy: 0.5017\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 30s 1s/step - loss: 0.7042 - accuracy: 0.4835\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 31s 1s/step - loss: 0.7120 - accuracy: 0.4851\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 30s 2s/step - loss: 0.6958 - accuracy: 0.5459\n"
     ]
    }
   ],
   "source": [
    "#this was trained on the balanced dataset\n",
    "models = trainDf(res,training_labels,wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words replaced ['infj']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 316s 2s/step - loss: 0.5489 - accuracy: 0.7601\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 327s 2s/step - loss: 0.5406 - accuracy: 0.7707\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 324s 2s/step - loss: 0.4165 - accuracy: 0.8428\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 310s 1s/step - loss: 0.3987 - accuracy: 0.8643\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 306s 1s/step - loss: 0.6966 - accuracy: 0.5130\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 306s 1s/step - loss: 0.6916 - accuracy: 0.5460\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 310s 1s/step - loss: 0.6786 - accuracy: 0.5992\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 311s 1s/step - loss: 0.6729 - accuracy: 0.6021\n",
      "words replaced ['intp']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 307s 1s/step - loss: 0.5443 - accuracy: 0.7727\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 307s 1s/step - loss: 0.5339 - accuracy: 0.7760\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 307s 1s/step - loss: 0.4130 - accuracy: 0.8633\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 320s 2s/step - loss: 0.3918 - accuracy: 0.8687\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 318s 2s/step - loss: 0.6970 - accuracy: 0.5278\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 318s 2s/step - loss: 0.6957 - accuracy: 0.5146\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 439s 2s/step - loss: 0.6773 - accuracy: 0.5972\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 364s 2s/step - loss: 0.6844 - accuracy: 0.5873\n",
      "words replaced ['infjs']\n",
      "training dim 1\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 326s 2s/step - loss: 0.5567 - accuracy: 0.7460\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 320s 2s/step - loss: 0.5379 - accuracy: 0.7733\n",
      "training dim 2\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 315s 1s/step - loss: 0.4154 - accuracy: 0.8435\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 319s 2s/step - loss: 0.3982 - accuracy: 0.8652\n",
      "training dim 3\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 324s 2s/step - loss: 0.6957 - accuracy: 0.5178\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 318s 2s/step - loss: 0.6919 - accuracy: 0.5327\n",
      "training dim 4\n",
      "Epoch 1/2\n",
      "209/209 [==============================] - 317s 2s/step - loss: 0.6806 - accuracy: 0.5866\n",
      "Epoch 2/2\n",
      "209/209 [==============================] - 316s 2s/step - loss: 0.6680 - accuracy: 0.6146\n"
     ]
    }
   ],
   "source": [
    "#this was trained on the entire dataset\n",
    "models = trainDf(res,training_labels,[[\"infj\"],[\"intp\"],[\"infjs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,row in enumerate(testing_posts):\n",
    "        df = pd.DataFrame(data={\"posts\":[row]})\n",
    "        print(tokenize(df))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elminated words: ['sweet', 'deal', 'month', 'then', 'know', 'thought', 'up.', 'off', 'asking', 'he', 'idea.', 'crush', 'this', 'college', 'it', 'most', 'dealing', 'late', 'key', 'his', 'talk', 'oh', 'those', 'everything.']\n",
      "accuracy for dim 1 personality classification = 0.40298507462686567\n",
      "accuracy for dim 2 personality classification = 0.5945273631840796\n",
      "accuracy for dim 3 personality classification = 0.472636815920398\n",
      "accuracy for dim 4 personality classification = 0.5447761194029851\n",
      "total accuracy for personality classification = 0.07213930348258707\n",
      "elminated words: ['loyalty:', 'dogmas', 'post.]', \"'rarity',\", 'static:', 'grins.', '358890', 'intimidate,', '84389', '84390', 'beck.', 'alpha;', 'transversal', 'ri,', 'gosh....this', 'now.....but', 'error(when', 'times(asshole', 'oooooh...', 'floss?', 'rupp,', 'customization.', 'backstories', 'error!!!,', 'time...there', 'yeah...this', 'jimmers,', 'boat?...', 'fi>ti>fe>te', '-plans', 'hulme', 'scotland?']\n",
      "accuracy for dim 1 personality classification = 0.40298507462686567\n",
      "accuracy for dim 2 personality classification = 0.5970149253731343\n",
      "accuracy for dim 3 personality classification = 0.47761194029850745\n",
      "accuracy for dim 4 personality classification = 0.4527363184079602\n",
      "total accuracy for personality classification = 0.07462686567164178\n"
     ]
    }
   ],
   "source": [
    "#this test was done on a balanced datset with significant words removed\n",
    "testDf(models,tokenize(testing_posts),testing_labels,wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elminated words: ['infj']\n",
      "accuracy for dim 1 personality classification = 0.40298507462686567\n",
      "accuracy for dim 2 personality classification = 0.5970149253731343\n",
      "accuracy for dim 3 personality classification = 0.5223880597014925\n",
      "accuracy for dim 4 personality classification = 0.5049751243781094\n",
      "total accuracy for personality classification = 0.05970149253731343\n",
      "elminated words: ['intp']\n",
      "accuracy for dim 1 personality classification = 0.40298507462686567\n",
      "accuracy for dim 2 personality classification = 0.40298507462686567\n",
      "accuracy for dim 3 personality classification = 0.47512437810945274\n",
      "accuracy for dim 4 personality classification = 0.5447761194029851\n",
      "total accuracy for personality classification = 0.022388059701492536\n",
      "elminated words: ['infjs']\n",
      "accuracy for dim 1 personality classification = 0.40298507462686567\n",
      "accuracy for dim 2 personality classification = 0.5970149253731343\n",
      "accuracy for dim 3 personality classification = 0.5223880597014925\n",
      "accuracy for dim 4 personality classification = 0.4552238805970149\n",
      "total accuracy for personality classification = 0.07462686567164178\n"
     ]
    }
   ],
   "source": [
    "#this test was done on a balanced datset\n",
    "testDf(models,tokenize(testing_posts),testing_labels,[[\"infj\"],[\"intp\"],[\"infjs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elminated words: ['infj']\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.54\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.2015\n",
      "elminated words: ['intp']\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5405\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.202\n",
      "elminated words: ['infjs']\n",
      "accuracy for dim 1 personality classification = 0.771\n",
      "accuracy for dim 2 personality classification = 0.86\n",
      "accuracy for dim 3 personality classification = 0.5405\n",
      "accuracy for dim 4 personality classification = 0.599\n",
      "total accuracy for personality classification = 0.202\n"
     ]
    }
   ],
   "source": [
    "#this test was done on imbalanced dataset\n",
    "testDf(models,tokenize(testing_posts),testing_labels,[[\"infj\"],[\"intp\"],[\"infjs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elminated words: ['infj']\n",
      "accuracy for dim 1 personality classification = 0.5970149253731343\n",
      "accuracy for dim 2 personality classification = 0.40298507462686567\n",
      "accuracy for dim 3 personality classification = 0.4900497512437811\n",
      "accuracy for dim 4 personality classification = 0.5447761194029851\n",
      "total accuracy for personality classification = 0.07711442786069651\n",
      "elminated words: ['intp']\n",
      "accuracy for dim 1 personality classification = 0.40298507462686567\n",
      "accuracy for dim 2 personality classification = 0.5845771144278606\n",
      "accuracy for dim 3 personality classification = 0.5223880597014925\n",
      "accuracy for dim 4 personality classification = 0.4552238805970149\n",
      "total accuracy for personality classification = 0.05721393034825871\n",
      "elminated words: ['infjs']\n",
      "accuracy for dim 1 personality classification = 0.5970149253731343\n",
      "accuracy for dim 2 personality classification = 0.40298507462686567\n",
      "accuracy for dim 3 personality classification = 0.47761194029850745\n",
      "accuracy for dim 4 personality classification = 0.4552238805970149\n",
      "total accuracy for personality classification = 0.07462686567164178\n"
     ]
    }
   ],
   "source": [
    "#this test was done on balanced dataset with less parameters\n",
    "testDf(models,tokenize(testing_posts),testing_labels,[[\"infj\"],[\"intp\"],[\"infjs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dim 1\n",
      "Epoch 1/5\n",
      "209/209 [==============================] - 434s 2s/step - loss: 0.5537 - accuracy: 0.7514\n",
      "Epoch 2/5\n",
      "209/209 [==============================] - 435s 2s/step - loss: 0.5443 - accuracy: 0.7681\n",
      "Epoch 3/5\n",
      " 55/209 [======>.......................] - ETA: 5:21 - loss: 0.5183 - accuracy: 0.7876"
     ]
    }
   ],
   "source": [
    "models = train4Dim(training_padded,training_labels,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50LEptZFKXl6",
    "outputId": "566e4359-729c-4d9b-9f93-8d0e8e1f7858"
   },
   "outputs": [],
   "source": [
    "# Only considering the top 10000 most common words\n",
    "vocab_size = 10000\n",
    "max_length = 2016\n",
    "# We only want to fit the tokenizer on the training, not the testing\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_posts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Puts the padding (which are 0) at the end of the vectorized sentence.\n",
    "# The longest post in our dataset is 2016, but we should truncate='post' earlier than 2016 words\n",
    "training_sequences = tokenizer.texts_to_sequences(training_posts)\n",
    "training_padded = pad_sequences(training_sequences, padding = 'post', maxlen = max_length)\n",
    "# training_sequences = np.array(training_sequences)\n",
    "training_padded = np.array(training_padded)\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_posts)\n",
    "testing_padded = pad_sequences(testing_sequences, padding = 'post', maxlen=max_length)\n",
    "# testing_sequences = np.array(testing_sequences)\n",
    "testing_padded = np.array(testing_padded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAccuracy(models,testing_labels,testing_padded):\n",
    "    for idx,model in enumerate(models):\n",
    "        test = testing_labels[idx]\n",
    "        modelOut = np.round(models[idx].predict(testing_padded))\n",
    "        print(f\"accuracy for dim {idx+1} personality classification = {np.mean(abs(np.squeeze(modelOut)-np.squeeze(test)) == 0)}\")\n",
    "#getAccuracy(models,testing_labels,testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalAccuracy(models,testing_labels,testing_padded):\n",
    "    total = None\n",
    "    isEmpty = True\n",
    "    for idx,model in enumerate(models):\n",
    "        test = testing_labels[idx]\n",
    "        modelOut = np.squeeze(np.round(models[idx].predict(testing_padded)))\n",
    "        if isEmpty:\n",
    "            total = np.array(modelOut)\n",
    "            isEmpty = False\n",
    "        else:\n",
    "            total = np.column_stack((total,modelOut))\n",
    "    labels = None\n",
    "    isEmpty = True\n",
    "    for idx,col in enumerate(testing_labels):\n",
    "        if isEmpty:\n",
    "            labels = np.array(col)\n",
    "            isEmpty = False\n",
    "        else:\n",
    "            labels = np.column_stack((labels,col))\n",
    "    print(f\"total accuracy for personality classification = {np.mean(np.sum(abs(total-labels),axis=1) == 0)}\")\n",
    "#getTotalAccuracy(models,testing_labels,testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4Dim(models,testing_padded,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AfPpLGS7MunO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Second parameter is the output dimension. Therefore, when we are changing this to predict 4 dimensions of personality we should change it to 4\n",
    "# ^^ actually i dont know if that is true\n",
    "embedding_dim = 256\n",
    "\n",
    "'''\n",
    "Embedding layer will always have vocab_size*embedding_dim parameters. Since vocab_size is 10,000 the number of parameters on this layer will always be large\n",
    "'''\n",
    "\n",
    "model = tf.keras.Sequential([ \n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.GRU(256, return_sequences=True),\n",
    "                            tf.keras.layers.SimpleRNN(128),\n",
    "                            tf.keras.layers.Dense(16, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = 'sgd', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6wi-9cvRtM6",
    "outputId": "30be599c-d625-4336-af86-7462bd9b982f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2016, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 2016, 256)         393984    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 3,005,328\n",
      "Trainable params: 3,005,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, training_labels, epochs = num_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/Desktop/cmput_466/project_scripts/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      " 128/1600 [=>............................] - ETA: 3:03 - loss: 2.8088 - acc: 0.0547"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpMLBWYmzYM5"
   },
   "outputs": [],
   "source": [
    "res = np.argmax(model.predict(testing_padded),axis=1)\n",
    "label = np.argmax(testing_labels,axis=1)\n",
    "print(f\"accuracy = {np.mean((label-res) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mbti_classifier_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
