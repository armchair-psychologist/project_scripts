{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#f = open(\"/kaggle/input/mbti-type/mbti_1.csv\", \"r\")\n#f.read()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport os\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nimport plotly\nimport seaborn as sns\n\n'''\nI used a slightly modified version of NaiveBayes classifier from AS1 here\n\n'''\n\nclass MyBayesClassifier():\n    def __init__(self, smooth=1):\n        self._smooth = smooth # This is for additive smoothing\n        \n\n    def train(self, X, y):\n        alpha_smooth = self._smooth\n        cls = np.unique(y)\n        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n\n        self._cls = cls\n        self._prior = np.zeros((1,Ncls))\n        #initialize a matrix stand for all p(x|y),the likelyhood for every attribute\n        self._likehood = np.zeros((Ncls,Nfeat))\n        #for each class,find rows that satisfies the condition,and compute the likehood of each feature\n        for i in range(Ncls):\n            cla = cls[i]\n            x_cla = X[y==cla]                                              #the rows that belong to current class\n            self._prior[0,i] =(x_cla.shape[0]+alpha_smooth)/(X.shape[0]+alpha_smooth*Ncls)                    #compute prior probability of current class\n            #self._likehood[i] = np.sum(x_cla,axis=0)/x_cla.shape[0]       #verticlly summation along each column to get frequency of each feature and then divide by # of rows\n\n            self._likehood[i,:] = (np.sum(x_cla, axis=0)+alpha_smooth) / (x_cla.shape[0]+ alpha_smooth * 2)                  #apply smooth to frequency divide # of rows\n            #print(self._likehood)\n            #print(self._prior)\n\n    def train_JM_smooth(self, X, y,X_dataset):\n        alpha_smooth = self._smooth\n        cls = np.unique(y)\n        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n\n        size_dataset = X_dataset.shape[0]             #number of data in dataset\n        feature_dataset = np.sum(X_dataset,axis=0)    #count the appearnce of each word in dataset by verticlly sum along each column from the dataset\n\n\n        self._cls = cls\n        self._prior = np.zeros((1,Ncls))\n        #initialize a matrix stand for all p(x|y),the likelyhood for every attribute\n        self._likehood = np.zeros((Ncls,Nfeat))\n        #for each class,find rows that satisfies the condition,and compute the likehood of each feature\n        for i in range(Ncls):\n            cla = cls[i]\n            x_cla = X[y==cla]                                              #the rows that belong to current class\n            self._prior[0,i] =x_cla.shape[0]/X.shape[0]                    #compute prior probability of current class\n            #self._likehood[i] = np.sum(x_cla,axis=0)/x_cla.shape[0]       #verticlly summation along each column to get frequency of each feature and then divide by # of rows\n            numerator1 = np.sum(x_cla, axis=0)\n            denominator1 = np.sum(x_cla)\n\n            self._likehood[i,:] = (1- alpha_smooth)*numerator1/denominator1 + alpha_smooth * feature_dataset / size_dataset                  #apply JM smooth to frequency divide # of rows\n\n        #after the for loop,self._prior stores the prior probability of all catergories and\n        #selef._likehood stores the probabality of all feature  such that  P(feature_i | catergory)\n            ###confusion  : smoothing????????\n        #self._notlikehood = 1-self._likehood              #P(not feature i | catergory )\n    def predict(self, X):\n\n        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]                         #number of test sample\n        pred = np.zeros(Ntest)\n        loglikehood = np.log(self._likehood)       \n        X_not = 1-X                   #since in original data,1 for appearence of feature i and 0 for not appear,the logic not means 1 for not appear and 0 for appear\n\n        #X is size Ntest x Nfeat,selef._likehood.T is the shape Nfeat x Ncls\n        log_appear = X.dot(np.log(self._likehood.T))\n        \n        #print(\"appear\",log_appear)\n        \n        log_absence = X_not.dot (np.log(1-self._likehood.T))\n        #print(\"absence\",log_absence)\n\n        log_post = log_appear+log_absence\n        #log_post = X.dot(np.log(self._likehood.T)) + X_not.dot (np.log(1-self._likehood.T))    #consider both appearence and absence\n        #log_post = X.dot(np.log(self._likehood.T))                                             #only consider appearence\n\n        log_post = log_post + np.log(self._prior.reshape(1,Ncls))\n        #print(log_post)\n        pred = self._cls[np.argmax(log_post,axis=1)]\n\n        return pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mbti-type/mbti_1.csv',header=0)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches(50, 20)\nsns.catplot(x=\"type\", kind=\"count\", data=df,height=8.27, aspect=11.7/8.27)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n# Just striping the string incase of any whitespace before or after the string\ndf[\"type\"] = df[\"type\"].str.strip()\n# Seperate the the label into four different parts\ntarget_multi_label = df[\"type\"].str.split(\"\" , expand=True)\ntarget_multi_label = target_multi_label.iloc[: , 1:-1]\ntarget_multi_label.columns = [\"Personality-1\",\"Personality-2\",\"Personality-3\",\"Personality-4\"]\n\ndf = pd.concat([df,target_multi_label] , axis=1)\n'''\npersonality_map = {\n    \"I\":\"Introvert\",\n    \"E\":\"Extrovert\",\n    \"N\":\"Intuitive\",\n    \"S\":\"Sensitive\",\n    \"F\":\"Emotional\",\n    \"T\":\"Thinker\",\n    \"J\":\"Judgemental\",\n    \"P\":\"Perceiving\"\n}\nfor col in df.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n    df[col] = df[col].map(personality_map)\n'''\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches(50, 20)\nsns.catplot(x=\"Personality-1\", kind=\"count\", data=df,height=5, aspect=4/5)\nsns.catplot(x=\"Personality-2\", kind=\"count\", data=df,height=5, aspect=4/5)\nsns.catplot(x=\"Personality-3\", kind=\"count\", data=df,height=5, aspect=4/5)\nsns.catplot(x=\"Personality-4\", kind=\"count\", data=df,height=5, aspect=4/5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#version1 of text pre-processing\n\n#source:https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n!pip install Unidecode\n!pip install contractions\n!pip install BeautifulSoup4\nimport nltk\nnltk.download('wordnet')\n\nfrom bs4 import BeautifulSoup\nimport spacy\nimport unidecode \n#from word2number import w2n\nimport contractions\nfrom nltk.stem import WordNetLemmatizer \nimport re\n\ndef preprocessing_v1(text):\n    #remove html information\n    soup = BeautifulSoup(text, \"html.parser\")\n    processed = soup.get_text(separator=\" \")\n    \n    #remove http// \n    processed = re.sub(r\"http\\S+\", \"\", processed)\n\n    #remove ||| seperate\n    processed = re.sub(r'\\|\\|\\|', r' ', processed)\n\n    #lower case\n    processed = processed.lower()\n\n    #expand shortened words, e.g. don't to do not\n    processed = contractions.fix(processed)\n\n    #remove accented char\n    processed = unidecode.unidecode(processed)\n\n    #remove white space\n    #processed = processed.strip()\n    #processed = \" \".join(processed.split())\n\n    # Lemmatizing \n    lemmatizer = WordNetLemmatizer() \n    processed=lemmatizer.lemmatize(processed)\n\n\n    return processed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['posts'] = df['posts'].apply(preprocessing_v1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split traning and test data and vectorize them\n\n\n\nnumber_training = 6000\ndata_size = df['type'].shape[0]\n\n\n\nall_data = df['posts'].astype('U').values\ndata_train = df['posts'][:number_training].astype('U').values\ndata_test = df['posts'][number_training:].astype('U').values\n\ny_train = df['type'][:number_training].astype('U').values\ny_test = df['type'][number_training:].astype('U').values\n\n\n#Note here, increase max_features may result in increasing ram usage and cause crush of colab\n#By defaut,it will geneate over 140000 features without any text preprocessing,it would decrease to near 100000 but still not acceptable\n#therefore I added a upper bound for max_features\nvectorizer = CountVectorizer(\n        lowercase=True, stop_words='english',\n        max_df=1.0, min_df=1, max_features=2000,  binary=True\n      )\nprocessed_data = vectorizer.fit_transform(all_data).toarray()\n\nX_train = processed_data[0:number_training, :]\nX_test = processed_data[number_training:, :]\n\nprint(\"X_train.shape = {}\".format(X_train.shape))\nprint(\"X_test.shape = {}\".format(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####   tempoary test cell , only used to debug some non-sense\nprint(X_train)\nprint(sum(X_train[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perform naive bayes,predict 1 among 16 personality types at once\nclf = MyBayesClassifier(1.0)\nclf.train(X_train, y_train);\ny_pred = clf.predict(X_test)\nprint(\"Absolute accuracy = {}\".format(np.mean(y_test==y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for each sub-personality type,train the model and make prediction\n#first test I/E,then N/S.......Cancadinate the result together to form final result\ny_pred = a2 = np.array(['' for i in range(data_size-number_training)])\nprint(y_pred.shape)\nprint(type(y_pred))\nclf = MyBayesClassifier(1.0)\n\nfor col in df.loc[: , \"Personality-1\":\"Personality-4\"].columns:\n    y_train_sub = df[col][:6000].astype('U').values\n    clf.train(X_train, y_train_sub);\n    y_pred_sub = clf.predict(X_test)\n\n    y_pred=np.core.defchararray.add(y_pred, y_pred_sub)\n\nprint(y_pred)\nprint(y_test)\nprint(\"Absolute accuracy = {}\".format(np.mean(y_test==y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this one is the early building version of LOOCV method\ndef LOOCV(k):\n    all_data = df['posts'].astype('U').values\n    #use int() to eliminate decimals\n    data_fragment_size = int(all_data / k)\n    #we vectorize the data first, but due to the ram overload, it is still an issue to be resolved\n    #for now, let's keep the max_feature limit\n    #the all_data should have the size of 2000 for now\n    vectorizer = CountVectorizer(\n        lowercase=True, stop_words=None,\n        max_df=1.0, min_df=1, max_features=2000,  binary=True\n    )\n    processed_data = vectorizer.fit_transform(all_data).toarray()\n\n    for i in range(0, k):\n        lower_bound = i * data_fragment_size\n        upper_bound = lower_bound + data_fragment_size\n        #split the data into training and testing based on k\n        #this part is just the modified version of the normal test part written by Zepeng Xiao\n\n        data_train = df['posts'][:lower_bound].astype('U').values + df['posts'][upper_bound:].astype('U').values\n        y_train = df['type'][:lower_bound].astype('U').values + df['posts'][upper_bound:].astype('U').values\n\n        data_test = df['posts'][lower_bound:upper_bound].astype('U').values\n        y_test = df['type'][lower_bound:upper_bound].astype('U').values\n\n        #not sure why we would both have data_train and x_train\n        #but I'll keep it like that anyway\n        x_train = processed_data[:lower_bound, :] + processed_data[upper_bound:, :]\n        x_test = processed_data[lower_bound:upper_bound, :]\n\n        #feed the data to the model and get the results\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntest for small Dset\n'''\ntest = CsvToDf(\"../input/mbti-type/mbti_1.csv\",batchSize=200)\nprint(test.getNextBatchCsv())\nres = test.getWholeCsv()\nprint(type(res))\nprint(test.getWholeCsv())\nprint(test.getHeader())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntest big dset\n\ntest2 = CsvToDf(\"../input/mbti-full-pull-samplecsv/mbti_full_pull_sample.csv\",batchSize=100,cols=['title','type'])\n#test2.eliminateCols(['created_utc', 'subreddit', 'author', 'domain', 'url', 'num_comments', 'score', 'ups', 'downs', 'selftext', 'saved', 'id', 'from_kind', 'gilded', 'from', 'stickied', 'retrieved_on', 'over_18', 'thumbnail', 'subreddit_id', 'hide_score', 'link_flair_css_class', 'author_flair_css_class', 'archived', 'is_self', 'from_id', 'permalink', 'name', 'author_flair_text', 'quarantine', 'link_flair_text', 'distinguished'])\nprint(type(test2.getNextBatchCsv()))\nprint(test2.getNextBatchCsv())\nprint(test2.getNextBatchCsv())\nprint(test2.getHeader())\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nTurn data into matrix\n'''\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef formatData(data,label,trainSize):\n    vectorizer = CountVectorizer(\n        lowercase=True, stop_words='english',\n        max_df=1.0, min_df=1, max_features=2000,  binary=True\n    )\n    out_data = vectorizer.fit_transform(data.astype('U').values).toarray()\n    out_label = label.astype('U').values\n    return (out_data[:trainSize],out_data[trainSize:],out_label[:trainSize],out_label[trainSize:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nformat data test\n'''\ntest = CsvToDf(\"../data/mbti_full_pull.csv\",cols=['body','subreddit'],batchSize=200)\ndata = test.getNextBatchCsv()\nxTrain,xTest,yTrain,yTest = formatData(data['body'],data['subreddit'],100)\nprint(xTrain)\nprint(yTrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Zepeng Xiao Version\n###In this version,we only need train data once to capture needed probability\n#could change smooth factor by cls._smooth = smoothfactor or update_smooth and call predict method\n####\nclass BayesClassifier_smooth():\n    def __init__(self, smooth=1):\n        self._smooth = smooth # This is for additive smoothing\n    \n    def update_smooth(i):\n        self._smooth = i\n    \n    #the train method would only count the probability now\n    def train(self, X, y):\n        alpha_smooth = self._smooth\n        cls = np.unique(y)\n        Ncls, Nfeat = len(cls), X.shape[1] #Ncls: number of classes, Nfeat: number of features.\n\n        self._train_size = X.shape[0]                           #store the number of training data\n        self._feature_count = np.sum(X,axis=0)                  #count the total appear time of all features(words) by vertically summation every column of training set\n        self._cls = cls                                         #store classes for predict use\n        self._prior = np.zeros((Ncls,1))                        #initialize a (Ncl * 1) shape matrix to store the count of each label,used to calculate prior probability later\n        self._word_count = np.zeros((Ncls,1))                   #initialize the (Ncl *1) matrix to store the total count of appearence of each word given class \n\n        self._likehood = np.zeros((Ncls,Nfeat))                 #initialize to store for all the count of each word given class\n                                                                #the number of rows equal to number of classes,column number equal to number of features(words)\n                                                                #therefore it will be used to calculate vectorlized likelyhood p(x|y) later\n\n        #for each class,find rows that satisfies the condition,and capture the count from training set so that it can be used to calculate probability\n        for i in range(Ncls):\n            cla = cls[i]                                                   #cla <---- current class\n            x_cla = X[y==cla]                                              #a subset of the rows that belong to current class\n            self._prior[i,0] =x_cla.shape[0]                               #count frequency of current class \n\n            #verticlly summation along each column to get frequency of each feature appear given current class\n            self._likehood[i,:] = np.sum(x_cla, axis=0)\n\n            #sum through the subset training data of current class,count the total number of appearence of all words,used for JMM smoothing later\n            self._word_count[i,:] = np.sum(x_cla)\n\n\n    #this method used for report the predict when using JM smooth approach for pard(d)\n    #parameter X is the training data,X_dataset is also used here to calculate the feature appreance count in whole dataset to avoid zero probability\n    def predict_JM(self,X,X_dataset):                       \n        alpha_smooth = self._smooth                               #the smooth parameter alpha\n        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]   #Ncls:number of class,Ntest:Number of test data,Nfeat:number of features\n        pred = np.zeros(Ntest)                                    #initialized the numpy array of predicted result,its size equals to number of test data\n        X_not = np.logical_not(X)                                 #for original data,1 for appearence of feature i and 0 for not appear,its logic not means 1 for not appear and 0 for appear\n\n                                                      #prior and likelyhood(probability) after additive smoothing\n        size_dataset = X_dataset.shape[0]             #number of data in dataset\n        feature_dataset = np.sum(X_dataset,axis=0)    #count the appearnce of each word in dataset by verticlly sum along each column from the dataset\n\n        #calculte prior probability by divide the count of class in training data by the training size\n        prior = self._prior / self._train_size\n\n        #compute the likelyhood probabity such that p(word i exist | class y) using JM smooth\n        likelyhood = (1-alpha_smooth)*(self._likehood/self._word_count) + (alpha_smooth) * (feature_dataset / size_dataset)\n        \n        #apply log transformation of likelyhood P(xi exists | y) and P(xi not exists | y)\n        not_likelyhood = np.log(1-likelyhood)\n        likelyhood = np.log(likelyhood)\n\n        #apply dot product to obtain the summation of log(P(xi exists | y)) and P(xi not exists | y)\n        log_appear = np.dot(X,likelyhood.T)\n        log_absence = np.dot (X_not,not_likelyhood.T)\n\n        #calculate log tranformed posterior probability\n        log_post = log_appear+log_absence + np.log(prior.reshape(1,Ncls))\n\n        #choose the y such that maximum the postior from the and return it\n        pred = np.argmax(log_post,axis=1)\n\n\n        return pred\n\n    def predict(self, X):\n        '''\n        This function has to return a numpy array of shape X.shape[0] (i.e. of shape \"number of testing examples\")\n        '''\n\n        alpha_smooth = self._smooth                                              #the smooth parameter alpha\n        Ncls,Ntest,Nfeat = len(self._cls),X.shape[0],X.shape[1]                  #Ncls:number of class,Ntest:Number of test data,Nfeat:number of features\n        pred = np.zeros(Ntest)                                                   #initialized the numpy array of predicted result,its size equals to number of test data\n        X_not = np.logical_not(X)                   #since in original data,1 for appearence of feature i and 0 for not appear,the logic not will give us the 1 for word not appear,and 0 for appear \n                                                    #used for vectorlizd multiplication(dot product) to get posterior\n        #calculte prior probability by divide the count of class in training data by the training size\n        prior = self._prior / self._train_size\n\n\n        #compute the likelyhood probabity such that p(word i exist | class y) using additive smooth\n        likelyhood = (self._likehood+alpha_smooth)/(self._prior + alpha_smooth*2)      #add alpha to numeriter and 2*alpha(cases of appear or not) to denominator\n\n\n        #X is in size Ntest x Nfeat, log(likehood.T) is the shape Nfeat x Ncls, each feature has 2 column in this case,each record its likelyhood of given y\n        #their product in shape Ntest x Ncls,for each test data,it has the summation of log(P(Xi exist | yi))\n      \n        log_appear = X.dot(np.log(likelyhood.T))\n\n        #1-likelyhood.T would generate all P(word i does not exist|class y),same size as likelyhood.T\n        #X_not dot product the log tranformation likelyhood for not appearence is the summation of log(P(xi does not exist | yi)) \n        log_absence = X_not.dot (np.log(1-likelyhood.T))\n\n        #add them up,it would equal the log transformed likelyhood used for naive bayes\n        log_post = log_appear+log_absence\n        \n        #add the log transformed prior probability,it become log transformed posterior probability\n        log_post = log_post + np.log(prior.reshape(1,Ncls))\n\n        #choose the y such that maximum the postior from the and return it\n        pred = self._cls[np.argmax(log_post,axis=1)]\n\n\n        return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport csv\n'''\nThis will load the csv\n'''\nclass CsvToDf:\n    '''\n    This class will simply turn the given data to a dataframe\n    '''\n    def __init__(self,filename,batchSize=None,cols=None):\n        #batchSize is the size of data to be read incrementally. This is for data that is to big to fit\n        #into memory\n        self._cols = cols\n        self._header = None\n        self._filename = filename\n        self._curIndex = 0     #this will be the current index that we are in the csv\n        self._isRead = False\n        self._df = None\n        self._storeHeader()\n        self._batchSize = batchSize\n    def _storeHeader(self):\n        with open(self._filename) as csvFile:\n            f = csv.reader(csvFile)\n            self._header = next(f)\n    def getWholeCsv(self):\n        if not(self._isRead):\n            if self._cols != None:\n                self._df = pd.read_csv(self._filename,usecols=self._cols)\n            else:\n                self._df = pd.read_csv(self._filename)\n            self._isRead = True\n        return self._df\n    def getHeader(self):\n        return self._header\n    def _checkIfRead(self):\n        if not(self._isRead):\n            if self._cols != None:\n                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize,usecols=self._cols)\n            else:\n                self._df = pd.read_csv(self._filename,iterator=True,chunksize=self._batchSize)\n            self._isRead = True\n            return False\n        return True\n    def getNextBatchCsv(self):\n        self._checkIfRead()\n        return next(self._df)","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = CsvToDf(\"../input/mbti-type/mbti_1.csv\",batchSize=100)\nbatch1 = data.getNextBatchCsv()\nbatch2 = data.getNextBatchCsv()\nprint(batch1)\nprint(batch2)\n","execution_count":2,"outputs":[{"output_type":"stream","text":"    type                                              posts\n0   INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n1   ENTP  'I'm finding the lack of me in these posts ver...\n2   INTP  'Good one  _____   https://www.youtube.com/wat...\n3   INTJ  'Dear INTP,   I enjoyed our conversation the o...\n4   ENTJ  'You're fired.|||That's another silly misconce...\n..   ...                                                ...\n95  INFP  'In Udaipur, Rajasthan, where I lived for a fe...\n96  INFJ  'Amelie - though I'm sure someone has mentione...\n97  INTJ  'https://www.youtube.com/watch?v=8IEQpfA528M  ...\n98  INFJ  'This.  Also flashbacks to times you've been c...\n99  ENFP  'Learning to say no, the right way, is the gre...\n\n[100 rows x 2 columns]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def formatData(data,label,trainSize):\n    vectorizer = HashingVectorizer(\n    lowercase=True, stop_words='english',binary=True)\n    out_data = vectorizer.fit_transform(data.astype('U').values).toarray()\n    out_label = label.str.lower().astype('U').values\n    return (out_data[:trainSize],out_data[trainSize:],out_label[:trainSize],out_label[trainSize:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strLabelToInt(inpLabel):\n    out = np.zeros((inpLabel.shape))\n    labels = ['estj', 'estp', 'esfj', 'esfp', 'entj', 'entp', 'enfj', 'enfp', 'istj', 'istp', 'isfj', 'isfp', 'intj', 'intp', 'infj', 'infp']\n    for idx,i in enumerate(inpLabel):\n        if i.lower() in labels:\n            out[idx] = labels.index(i)\n        else:\n            out[idx] = -100\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNB classifier that can train on batches\n'''\nclass BayesClassifier_smooth():\n    def __init__(self,smooth=1):\n        self._smooth = smooth # This is for additive smoothing\n        self._cls = ['estj', 'estp', 'esfj', 'esfp', 'entj', 'entp', 'enfj', 'enfp', 'istj', 'istp', 'isfj', 'isfp', 'intj', 'intp', 'infj', 'infp']\n        self._prior = np.zeros((16,1))                        #initialize a (Ncl * 1) shape matrix to store the count of each label,used to calculate prior probability later\n        self._likehood = None\n        self._notInit = True\n        self._prob_matrix = None\n        #given test dataset we need to reshape likelihood to size of test data\n    def update_smooth(i):\n        self._smooth = i\n        \n    #the train method would only count the probability now\n    def train(self, X, y):\n        cls = np.unique(y)\n        Nfeat = X.shape[1] #Nfeat: number of features.\n        if self._notInit:\n            self._likehood = np.zeros((16,Nfeat))\n            self._notInit = False\n        for i in range(16):\n            cla = self._cls[i]                                                #cla <---- current class\n            x_cla = X[y==cla]                                              #a subset of the rows that belong to current class\n            self._prior[i,0] += x_cla.shape[0]                               #count frequency of current class \n            #verticlly summation along each column to get frequency of each feature appear given current class\n            self._likehood[i,:] += np.sum(x_cla, axis=0)\n\n\n    #this method used for report the predict when using JM smooth approach for pard(d)\n    #parameter X is the training data,X_dataset is also used here to calculate the feature appreance count in whole dataset to avoid zero probability\n    def _compute_prediction(self,data_point):\n        '''\n        precondition: datapoint must be a list of integers that are either 0 or 1. And its length must be the same as the number of features.\n        postcondition: no side effects\n        this will return an array of 16 floats [0,1] each corresponding to one of the possible personality types\n        '''\n        data_point_matrix = np.array([data_point,]*self._prob_matrix.shape[0])\n        true_matrix = data_point_matrix * self._prob_matrix\n        false_matrix = ((data_point_matrix+1)%2) * (1-self._prob_matrix)\n        label_array = np.squeeze(np.sum(np.log(true_matrix+false_matrix),axis=1))\n        prior_array = np.squeeze(np.log((self._prior+self._smooth)/(16*self._smooth+sum(self._prior))))\n        return np.squeeze(label_array+prior_array)\n    def predict(self, X):\n        '''\n        This function has to return a numpy array of shape X.shape[0] (i.e. of shape \"number of testing examples\")\n        '''\n        self._prob_matrix = (self._likehood+self._smooth)/(self._prior + self._smooth*2)\n        #For each row in X determine the likelihood of it being one of the personalities\n        result = np.apply_along_axis(self._compute_prediction,axis=1,arr=X)\n        #result will be a matrix that has the shape of (# rows in X,# of possible personalities).\n        #each entry will represent the probability of a data point being a specific personality\n        pred = np.argmax(result,axis=1)\n\n        return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNaive Bayes Test 1\n'''\nclassifier = BayesClassifier_smooth()\ntest = CsvToDf(\"../input/mbti-type/mbti_1.csv\",batchSize=200,cols=['posts', 'type'])\nprint(test.getHeader())\ndata = test.getNextBatchCsv()\nxTrain,xTest,yTrain,yTest = formatData(data['posts'],data['type'],100)\nyTest = strLabelToInt(yTest)\nclassifier.train(xTrain,yTrain)\npred = classifier.predict(xTest)\nprint(\"accuracy = {}\".format(np.mean((yTest-pred)==0)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}